{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from rdflib import Graph, URIRef\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import json\n",
    "import time\n",
    "import atexit\n",
    "import getpass\n",
    "import requests  # install the package via \"pip install requests\"\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "import rdflib\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from statistics import mode\n",
    "\n",
    "import time\n",
    "import atexit\n",
    "import getpass\n",
    "import requests  # install the package via \"pip install requests\"\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#from sparknlp.annotator import *\n",
    "#from sparknlp.base import *\n",
    "#import sparknlp\n",
    "#import spark\n",
    "import urllib.request\n",
    "\n",
    "#from sparknlp.annotator import *\n",
    "#from sparknlp.base import *\n",
    "#import sparknlp\n",
    "#spark = sparknlp.start()\n",
    "\n",
    "#from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "###\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Answer factual questions\n",
    "Model training need to be applied only once. The function is at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NerTrainer:\n",
    "    #Most of the funcitons are from the Graphical Models Tutorial from the class\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_train = pd.read_csv('engtrain_clean.csv', encoding=\"ISO-8859-1\")\n",
    "        self.x_test = pd.read_csv('engtest_clean.csv', encoding=\"ISO-8859-1\")\n",
    "        print(\"Loaded data & initialized NER-model\")\n",
    "\n",
    "    def collate(self, dataframe):\n",
    "        agg_func = lambda s: [(w, pos, t) for w, pos, t in\n",
    "                              zip(s['WORD'].values.tolist(), s['POS'].values.tolist(), s['ï»¿TAG'].values.tolist())]\n",
    "        grouped = dataframe.groupby('SENTENCE').apply(agg_func)\n",
    "        return list(grouped)\n",
    "\n",
    "    # counts sentences in given input data\n",
    "    def count_sentences(self, data):\n",
    "        counter = 0\n",
    "        for index, row in data.iterrows():\n",
    "            data.at[index, 'SENTENCE'] = counter\n",
    "            if pd.isna(data.iloc[index, 0]):\n",
    "                counter += 1\n",
    "        data[\"POS\"] = \"0\"\n",
    "        data = data.dropna()\n",
    "        data = data.reset_index(drop=True)\n",
    "        return data\n",
    "\n",
    "    def word2features(self, sent, i):\n",
    "        word = sent[i][0]\n",
    "        postag = sent[i][1]\n",
    "\n",
    "        features = {\n",
    "            'word.lower()': word.lower(),  # the word in lowercase\n",
    "            'word[-3:]': word[-3:],  # last three characters\n",
    "            'word[-2:]': word[-2:],  # last two characters\n",
    "            'word.isupper()': word.isupper(),  # true, if the word is in uppercase\n",
    "            'word.istitle()': word.istitle(),\n",
    "            # true, if the first character is in uppercase and remaining characters are in lowercase\n",
    "            'word.isdigit()': word.isdigit(),  # true, if all characters are digits\n",
    "            'postag': postag,  # POS tag\n",
    "            'postag[:2]': postag[:2],  # IOB prefix\n",
    "        }\n",
    "\n",
    "        if i > 0:\n",
    "            word1 = sent[i - 1][0]  # the previous word\n",
    "            postag1 = sent[i - 1][1]  # POS tag of the previous word\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2],\n",
    "            })  # add some features of the previous word\n",
    "        else:\n",
    "            features['BOS'] = True  # BOS: begining of the sentence\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i + 1][0]  # the next word\n",
    "            postag1 = sent[i + 1][1]  # POS tag of the next word\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2],\n",
    "            })  # add some features of the next word\n",
    "        else:\n",
    "            features['EOS'] = True  # EOS: end of the sentence\n",
    "        return features\n",
    "\n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for _, _, label in sent]\n",
    "\n",
    "    def tag_data(self, data_to_tag):\n",
    "        tags = []\n",
    "        x_train = data_to_tag.groupby(['SENTENCE'])\n",
    "        x_train = x_train.apply(np.mean)\n",
    "\n",
    "        # x_train.shape[0] will give us the number of sentences to loop through and tag them using nltk lib\n",
    "        # first we locate the sentence in our input data, then we hand the sentence as a list of words to the nltk function which tags the words.\n",
    "        for i in range(x_train.shape[0]):\n",
    "            sentence = data_to_tag.loc[data_to_tag[\"SENTENCE\"] == i]\n",
    "            sentence = sentence['WORD'].tolist()\n",
    "            tag = nltk.pos_tag(sentence)\n",
    "            tags.append(tag)\n",
    "\n",
    "        #now we add the found tags to the actual data in the data set\n",
    "        counter = 0\n",
    "        for sentence in tags:\n",
    "            for word in sentence:\n",
    "                data_to_tag.at[counter, 'POS'] = word[1]\n",
    "                counter += 1\n",
    "        print(\"data to tag in \"\"tag_data()\"\":\" )\n",
    "        print(data_to_tag)\n",
    "        print(\"#######################################################\")\n",
    "        return data_to_tag\n",
    "\n",
    "    #code copied from graphical_model tutorial and collected under one train function\n",
    "    def train(self):\n",
    "        self.x_train = self.count_sentences(self.x_train)\n",
    "        self.x_test = self.count_sentences(self.x_test)\n",
    "\n",
    "        self.x_train = self.tag_data(self.x_train)\n",
    "        self.x_test = self.tag_data((self.x_test))\n",
    "\n",
    "        self.x_train = self.collate(self.x_train)\n",
    "        self.x_test = self.collate((self.x_test))\n",
    "\n",
    "        X_train, y_train = [self.sent2features(s) for s in self.x_train], [self.sent2labels(s) for s in self.x_train]\n",
    "        X_test, y_test = [self.sent2features(s) for s in self.x_test], [self.sent2labels(s) for s in self.x_test]\n",
    "\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='l2sgd',  # l2sgd: Stochastic Gradient Descent with L2 regularization term\n",
    "            max_iterations=1000,  # maximum number of iterations\n",
    "        )\n",
    "        crf_model = crf.fit(X_train, y_train)\n",
    "        return  crf_model\n",
    "\n",
    "    #predict tags of given input sentence\n",
    "    def predict_tags(self, model, sentence):\n",
    "        text = []\n",
    "\n",
    "        #create list of workds from input sentence\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        text.append(tagged_words)\n",
    "\n",
    "        empty_list = word_tokenize(\"\")\n",
    "        empty_tagged_words = nltk.pos_tag(empty_list)\n",
    "        text.append(empty_tagged_words)\n",
    "\n",
    "        X_train = [self.sent2features(s) for s in text]\n",
    "        y = model.predict(X_train)\n",
    "\n",
    "        #remove any punctuation mark\n",
    "        if sentence[0][-1][0] in ['?', '!', '¨.']:\n",
    "            y[0][-1] = '0'\n",
    "\n",
    "        #Create output dataframe with two new columns and the predicted tag\n",
    "        tagged_words = np.asarray(tagged_words)\n",
    "        df_res = pd.DataFrame(tagged_words, columns=['WORD', 'POS'])\n",
    "        df_res['TAG'] = y[0]\n",
    "\n",
    "        return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class to identify the input question. Question content and type.\n",
    "class QuestionAnalyser:\n",
    "    def __init__(self):\n",
    "        print(\"Initialized Question Analyser\")\n",
    "        self.exceptional_predicates = {'MPA film rating':\"P1657\", 'Box Office Mojo film ID':\"P1237\", 'duration':\"P2047\", 'FilmAffinity ID':\"P480\", \"cast member\":\"P161\",\n",
    "                                       'Kinopoisk film ID':\"P2603\", \"producer\":'P162', 'exploitation visa number':\"P2755\", 'average shot length':\"P2208\", 'film script':\"P3816\", 'budget':\"P2769\"}\n",
    "\n",
    "    #Funciton to identify if question is about the director ot the actor.\n",
    "    #Counts nouns and verbs that are related to either. Role with highest score is designated as question content.\n",
    "    def identity_counter(self, lemma_verbs, lemma_nouns):\n",
    "\n",
    "        lemma_words = lemma_verbs + lemma_nouns\n",
    "\n",
    "        candidates = {\"director\" : ['director', \"filmmaker\",'direct', 'produce', 'filmed', 'lead'],\n",
    "                      \"actor\" : ['actor', 'role', 'star', 'act', 'play', 'star'],\n",
    "                      \"screenwriter\":  ['screenwriter', 'scriptwriter', 'author', 'screenwrite', 'write', 'script'],\n",
    "                      \"MPA film rating\" : ['mpaa', 'rating', 'ranking', 'grade', 'rate', 'rank', 'grade'],\n",
    "                      \"image\" : ['image', 'picture', 'face', 'look', \"show\"],\n",
    "                      \"genre\" : ['genre', 'type', 'style', 'category'],\n",
    "                      \"box office\" : [\"box\", \"office\", \"ticket\", \"booth\"],\n",
    "                      \"publication date\": [\"publication\", \"publish\", \"release\"],\n",
    "                      \"cast member\": [\"cast\", \"casting\"],\n",
    "                      \"producer\": [\"producer\", 'produced', \"produce\", \"maker\", \"make\", \"production\"],\n",
    "                      \"executive producer\": [\"producer\",'produced', \"produce\", \"executive\", \"maker\", \"make\"],\n",
    "                      \"based on\": [\"base\", \"based\"],\n",
    "                      \"country of origin\" : [\"origin\"],\n",
    "                      \"filming location\": [\"location\", \"place\", \"locate\", \"film\", \"shoot\"],\n",
    "                      \"narrative location\": [\"location\", \"place\", \"locate\", \"narrate\", \"narrative\"],\n",
    "                      \"director of photography\": [\"photography\"],\n",
    "                      \"Box Office Mojo film ID\": [\"box\", \"office\", \"ticket\", \"booth\", \"Mojo\", \"ID\", \"id\"],\n",
    "                      \"set in period\": [\"period\"],\n",
    "                      \"duration\": [\"duration\"],\n",
    "                      \"film editor\": [\"editor\", \"edit\", \"compiler\", \"compile\"],\n",
    "                      \"FilmAffinity ID\": [\"FilmAffinity\", \"filmaffinity\"],\n",
    "                      \"Filmportal\": [\"Filmportal\", \"filmportal\"],\n",
    "                      \"main subject\": [\"subject\", \"topic\"],\n",
    "                      \"composer\": [\"composer\", \"compose\"],\n",
    "                      \"distributed by\": [\"distributer\", \"distribute\", \"trader\", \"trade\", \"seller\"],\n",
    "                      \"costume designer\": [\"costume\", \"outfit\", \"wardrobe\", \"dress\"],\n",
    "                      \"production designer\": [\"producer\",'produced', \"produce\", \"executive\", \"maker\", \"make\", \"designer\", \"design\"],\n",
    "                      \"Kinopoisk film ID\": [\"Kinopoisk\", \"kinopoisk\"],\n",
    "                      \"average shot length\": [\"average\", \"shot\", \"length\"],\n",
    "                      \"exploitation visa number\": [\"visa\", \"exploitation\"],\n",
    "                      \"original film format\": [\"original\", \"film\", \"format\"],\n",
    "                      \"film script\": [\"script\", \"scripte\"],\n",
    "                      \"title\": [\"title\"],\n",
    "                      \"award received\": [\"award\"],\n",
    "                      \"media franchise\": [\"franchise\"],\n",
    "                      \"part of the series\": [\"series\"],\n",
    "                      \"budget\": [\"budget\", \"cost\"],\n",
    "\n",
    "                      ############################\n",
    "\n",
    "                      }\n",
    "\n",
    "\n",
    "        counter = {\"director\": 0, \"actor\": 0, \"screenwriter\": 0, \"MPA film rating\": 0, \"image\": 0, \"genre\": 0,\n",
    "                   \"box office\": 0, \"publication date\": 0, \"cast member\": 0, \"producer\": 0, \"executive producer\":0,\n",
    "                   \"based on\": 0, \"country of origin\": 0, \"filming location\": 0, \"narrative location\": 0, \"director of photography\": 0,\n",
    "                   \"Box Office Mojo film ID\":0, \"set in period\": 0, \"duration\": 0, \"film editor\": 0, \"FilmAffinity ID\": 0,\n",
    "                   \"Filmportal ID\": 0, \"main subject\": 0, \"composer\": 0, \"distributed by\": 0,  \"costume designer\": 0,\n",
    "                   \"production designer\": 0, \"Kinopoisk film ID\": 0, \"exploitation visa number\": 0, \"average shot length\": 0,\n",
    "                   \"original film format\": 0, \"film script\": 0, \"title\": 0, \"award\": 0, \"media franchise\": 0, \"part of the series\": 0,\n",
    "                   \"budget\": 0\n",
    "                   }\n",
    "\n",
    "        #check occurences of words in input sentence\n",
    "        for key in candidates.keys():\n",
    "            for w in candidates[key]:\n",
    "                if w in lemma_words:\n",
    "                    counter[key] += 1\n",
    "\n",
    "        max_value = max(counter, key=counter.get)\n",
    "\n",
    "        #double check for box office and avg shot length since it should appear in word combinations\n",
    "        if max_value == \"Box Office Mojo film ID\":\n",
    "            lemma_lower = [w.lower() for w in lemma_words]\n",
    "            if \"id\" not in lemma_lower:\n",
    "                max_value = \"box office\"\n",
    "\n",
    "        if max_value == \"average shot length\":\n",
    "            counter[\"average shot length\"] -= 1\n",
    "            max_value = max(counter, key=counter.get)\n",
    "        if max_value == \"original film format\":\n",
    "            counter[\"original film format\"] -= 1\n",
    "            max_value = max(counter, key=counter.get)\n",
    "\n",
    "\n",
    "        #decide between location properties:\n",
    "        if max_value in [\"country of origin\", \"filming location\", \"narrative location\"]:\n",
    "            if \"origin\" in lemma_verbs:\n",
    "                max_value = \"country of origin\"\n",
    "            elif \"film\" or \"shoot\" in lemma_verbs:\n",
    "                max_value = \"filming location\"\n",
    "            else:\n",
    "                max_value = \"narrative location\"\n",
    "\n",
    "        #decide between the production rolees:\n",
    "        if max_value in [\"production designer\", \"executive producer\"]:\n",
    "            if \"designer\" in lemma_words:\n",
    "                max_value = \"production designer\"\n",
    "            elif  \"design\" in lemma_words:\n",
    "                max_value = \"production designer\"\n",
    "            elif \"executive\" in lemma_words:\n",
    "                max_value = \"executive producer\"\n",
    "            else: max_value = \"producer\"\n",
    "\n",
    "        print(\"max valu in identity counter: \")\n",
    "        print(max_value, counter[max_value])\n",
    "\n",
    "\n",
    "        if counter[max_value] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return max_value\n",
    "\n",
    "    def identify_predicate(self, question, recurrence = False):\n",
    "\n",
    "        # Get untaggde words\n",
    "        untagged_words = question[question[\"TAG\"]=='0']\n",
    "        verbs = untagged_words[untagged_words['POS'].isin(['VP', 'VBD', 'VBG', 'VBP', 'VBZ'])]['WORD']\n",
    "        nouns = untagged_words[untagged_words['POS'].isin(['NN', 'NNP', 'NNPS', \"JJ\"])]['WORD']\n",
    "\n",
    "        #Lemmatize means to get the root of the word\n",
    "        lemma = WordNetLemmatizer()\n",
    "        lemma_verbs = [lemma.lemmatize(v) for v in verbs]\n",
    "        lemma_nouns = [lemma.lemmatize(v) for v in nouns]\n",
    "        print(lemma_nouns)\n",
    "        print(lemma_verbs)\n",
    "\n",
    "        ########################################################\n",
    "        pred = self.identity_counter(lemma_nouns = lemma_nouns, lemma_verbs=lemma_verbs)\n",
    "\n",
    "        #Sometimes we find that the predicate is bound to the tile by tags. Is we cant find a pred,\n",
    "        #We remove the tag from every word that occurs before a \"of\" word that was tagged.\n",
    "        if recurrence == False:\n",
    "            if pred == False:\n",
    "                looks = [\"look\", \"looks\"]\n",
    "                if any(c in question[\"WORD\"].unique() for c in looks):\n",
    "                    try:\n",
    "                        idx = question.index[question['WORD'] == \"look\"].tolist()\n",
    "                        question.iloc[idx[0]:][\"TAG\"] = '0'\n",
    "                        print(\"changed look like tags\")\n",
    "                        print(question)\n",
    "                        pred = self.identify_predicate(question, recurrence=True)\n",
    "\n",
    "                    except:\n",
    "                        try:\n",
    "                            idx = question.index[question['WORD'] == \"looks\"].tolist()\n",
    "                            question.iloc[idx[0]:][\"TAG\"] = '0'\n",
    "                            print(\"changed look like tags\")\n",
    "                            print(question)\n",
    "                            pred = self.identify_predicate(question, recurrence=True)\n",
    "                        except:\n",
    "                            pred = False\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        idx = question.index[question['WORD'] == \"of\"].tolist()\n",
    "                        question.iloc[:idx[0]+1][\"TAG\"] = '0'\n",
    "                        print(\"changed tags\")\n",
    "                        print(question)\n",
    "                        pred = self.identify_predicate(question, recurrence=True)\n",
    "                    except:\n",
    "                        pred = False\n",
    "        # check if start of usbject could be identified. If not, set in manually-\n",
    "        if pred == False:\n",
    "            print(\"changing tags\")\n",
    "            if \"B-TITLE\" not in question[\"TAG\"].unique():\n",
    "                print(\"changning Tags because no title tags found\")\n",
    "                try:\n",
    "                    idx = question.index[question[\"TAG\"]==\"I-PLOT\"].tolist()\n",
    "                    question.iloc[idx[0]:idx[0]+1][\"TAG\"] = 'B-TITLE'\n",
    "                    question.loc[question['TAG'] == \"I-PLOT\", 'TAG'] = 'I-TITLE'\n",
    "                    pred = self.identify_predicate(question, recurrence = True)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        print(\"pred to return:\")\n",
    "        print(pred)\n",
    "        return pred\n",
    "\n",
    "    def handle_exceptional_predicates(self, predicate):\n",
    "        print(\"excepional predicate triggered\")\n",
    "        return self.exceptional_predicates[predicate]\n",
    "\n",
    "    def predicate_corrector(self, question):\n",
    "        looks = [\"look\", \"looks\"]\n",
    "        if any(c in question[\"WORD\"].unique() for c in looks):\n",
    "            try:\n",
    "                idx = question.index[question['WORD'] == \"look\"].tolist()\n",
    "                question.iloc[idx[0]:][\"TAG\"] = '0'\n",
    "                print(\"changed look like tags\")\n",
    "                print(question)\n",
    "                pred = self.identify_predicate(question, recurrence=True)\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    idx = question.index[question['WORD'] == \"looks\"].tolist()\n",
    "                    question.iloc[idx[0]:][\"TAG\"] = '0'\n",
    "                    print(\"changed look like tags\")\n",
    "                    print(question)\n",
    "                    pred = self.identify_predicate(question, recurrence=True)\n",
    "                except:\n",
    "                    pred = False\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                idx = question.index[question['WORD'] == \"of\"].tolist()\n",
    "                question.iloc[:idx[0]+1][\"TAG\"] = '0'\n",
    "                print(\"changed tags\")\n",
    "                print(question)\n",
    "                pred = self.identify_predicate(question, recurrence=True)\n",
    "            except:\n",
    "                pred = False\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaaaaaX\n"
     ]
    }
   ],
   "source": [
    "r = \"BaaaaaX \"\n",
    "\n",
    "if r[-1].isspace():\n",
    "    print(r[:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TypoCorrector:\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_last_digit_space(input):\n",
    "        # replace minus with dash\n",
    "        if input[-1].isspace():\n",
    "            input = input[:-1]\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def change_minus_sign(input):\n",
    "        # replace minus with dash\n",
    "        if \"-\" in input:\n",
    "            input = input.replace(\"-\", \"–\")\n",
    "\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def change_dash_sign(input):\n",
    "        # replace minus with dash\n",
    "        if \"-\" in input:\n",
    "            input = input.replace(\"–\",\"-\")\n",
    "\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def title_corrector(film, actor):\n",
    "\n",
    "        # To perform the query, we must remove punctuations.\n",
    "        if len(film) >= 1:\n",
    "            if film[-1] in ['?', '!', '.']:\n",
    "                film = film[:-1]\n",
    "        # intermediate punctions have a space in front of them that needs to be removed\n",
    "        if \" :\" in film:\n",
    "            film = film.replace(\" :\", \":\")\n",
    "\n",
    "        if len(actor) >= 1:\n",
    "            if actor[-1] in ['?', '!', '.']:\n",
    "                actor = actor[:-1]\n",
    "\n",
    "        return film, actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class  ModelTrainer:\n",
    "\n",
    "    def __init__(self, ner_model, training_data, test_data):\n",
    "        self.ner_model = ner_model\n",
    "        self.x_train = pd.read_csv(training_data, encoding=\"ISO-8859-1\")\n",
    "        self.x_test = pd.read_csv(test_data, encoding=\"ISO-8859-1\")\n",
    "        print(\"Loaded data successfully\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        self.ner_model.train(self.x_train,self.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Crowdsourcing:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv('D:/deanh/Uni/Master/HS22/Advanced AI/Project/crowdsourcing/crowd_data.tsv', sep='\\t')\n",
    "        self.df = self.clean_crowdsourcingfile()\n",
    "        self.kappas = self.calculate_kappas(self.df)\n",
    "\n",
    "\n",
    "    def clean_crowdsourcingfile(self):\n",
    "        #remove % sign from approvalrate to handle as integer\n",
    "        self.df[\"LifetimeApprovalRate\"] = [int(word[:-1]) for word in self.df['LifetimeApprovalRate']]\n",
    "\n",
    "        # We define upper and lower bounds for response time and approval rate by using mean and standard deviation\n",
    "        # In believe that workers outside of these bounds are malicious. We add +5% to all bounds to include valid edge cases\n",
    "        # We also drop rows that indicate as CORRECT but say the know an error and where to handle it\n",
    "\n",
    "        x = \"WorkTimeInSeconds\"\n",
    "        y = \"LifetimeApprovalRate\"\n",
    "\n",
    "        # mean_approvalRate = self.df[y].mean()\n",
    "        # std_approvalRate = self.df[y].std()\n",
    "        #\n",
    "        # mean_responseTime = self.df[x].mean()\n",
    "        # std_responseTime = self.df[x].std()\n",
    "        #\n",
    "        # upperbound_responseTime = mean_responseTime + std_responseTime + 5\n",
    "        # lowerbound_responseTime = mean_responseTime - std_responseTime - 5\n",
    "        #\n",
    "        # upperbound_approvalRate = mean_approvalRate + std_approvalRate + 5\n",
    "        # lowerbound_approvalRate = mean_approvalRate - std_approvalRate -5\n",
    "        #\n",
    "        # self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds > upperbound_responseTime)].index)\n",
    "        # self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds < lowerbound_responseTime)].index)\n",
    "        #\n",
    "        # self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate > upperbound_approvalRate)].index)\n",
    "        # self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate < lowerbound_approvalRate)].index)\n",
    "\n",
    "        ###################################################################################################################\n",
    "        workers = pd.DataFrame(self.df[\"WorkerId\"])\n",
    "        malicous_workers_Id = []\n",
    "\n",
    "        for e in workers[\"WorkerId\"]:\n",
    "            df_n = pd.DataFrame(self.df.loc[self.df[\"WorkerId\"] == e, \"AnswerID\"])\n",
    "            if len(df_n[\"AnswerID\"].unique()) != 2:\n",
    "                malicous_workers_Id.append(e)\n",
    "\n",
    "        for e in malicous_workers_Id:\n",
    "            self.df = self.df.drop(self.df[(self.df.WorkerId == e)].index)\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds <= 5)].index)\n",
    "        self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate <= 40)].index)\n",
    "\n",
    "\n",
    "        #There is a type in Input2ID column\n",
    "        self.df[\"Input2ID\"] = self.df[\"Input2ID\"].replace(\"wdt:.P344\", \"wdt:P344\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def crowdsourcing_answer(self, sub, pred):\n",
    "        rows = self.df.loc[self.df[\"Input1ID\"] == \"wd:\"+sub]\n",
    "        rows = rows.loc[rows[\"Input2ID\"] == \"wdt:\"+pred]\n",
    "\n",
    "        agree = rows.loc[rows[\"AnswerLabel\"] == \"CORRECT\"]\n",
    "        disagree = rows.loc[rows[\"AnswerLabel\"] == \"INCORRECT\"]\n",
    "        value = rows.iloc[0]['Input3ID']\n",
    "        batch_id = rows.iloc[0][\"HITTypeId\"]\n",
    "        kappa = self.kappas[batch_id]\n",
    "\n",
    "        answer = {\"batch\": batch_id, \"kappa\": kappa, \"agreement\": agree.shape[0], \"disagreement\":disagree.shape[0], \"value\" : str(value), \"answer\": \"tie\"}\n",
    "        if agree.shape[0] > disagree.shape[0]:\n",
    "            answer[\"verdict\"] = \"True\"\n",
    "        if agree.shape[0] < disagree.shape[0]:\n",
    "            answer[\"verdict\"] = \"False\"\n",
    "            for e in disagree[\"FixValue\"].unique():\n",
    "                if isinstance(e, str):\n",
    "                    if e != \"I don't understand\":\n",
    "                        answer[\"value\"] = e\n",
    "                        answer[\"verdict\"] = \"corrected\"\n",
    "        if agree.shape[0] == disagree.shape[0]:\n",
    "            answer[\"verdict\"] = \"tie\"\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def calculate_kappas(self, df):\n",
    "\n",
    "        batch_names = df[\"HITTypeId\"].unique()\n",
    "        voting_list = {}\n",
    "        for batch_name in batch_names:\n",
    "            batch = df.loc[df[\"HITTypeId\"] == batch_name]\n",
    "            voters = batch[\"WorkerId\"].unique()\n",
    "            voting_collection = [[] for v in range(len(voters))]\n",
    "\n",
    "            for i in range (len(voters)):\n",
    "                worker = voters[i]\n",
    "                votes = batch.loc[batch[\"WorkerId\"] == worker][\"AnswerLabel\"].tolist()\n",
    "                voting_collection[i] = (votes)\n",
    "            voting_list[batch_name] = voting_collection\n",
    "\n",
    "        kappas = {}\n",
    "        for key in voting_list.keys():\n",
    "            vote_values = voting_list[key]\n",
    "        #pe\n",
    "            correct = 0\n",
    "            incorrect = 0\n",
    "            for e in vote_values:\n",
    "                for i in e:\n",
    "                    if i == \"CORRECT\":\n",
    "                        correct += 1\n",
    "                    elif i == \"INCORRECT\":\n",
    "                        incorrect += 1\n",
    "            sum = correct + incorrect\n",
    "            pe = (correct/sum)**2 + (incorrect/sum)**2\n",
    "\n",
    "            #po\n",
    "            N = len(vote_values[0])\n",
    "            n = len(vote_values)\n",
    "            first_part = 1 / (N*n*(n-1))\n",
    "\n",
    "            sec_part = 0\n",
    "            for i in range(len(vote_values[0])):\n",
    "                correct_counter = 0\n",
    "                incorrect_counter  = 0\n",
    "                for e in range(len(vote_values)):\n",
    "                    if vote_values[e][i] == \"CORRECT\":\n",
    "                        correct_counter += 1\n",
    "                    elif vote_values[e][i] == \"INCORRECT\":\n",
    "                        incorrect_counter += 1\n",
    "\n",
    "                sec_part += correct_counter**2\n",
    "                sec_part += incorrect_counter**2\n",
    "\n",
    "            po =  first_part * (sec_part - N * n)\n",
    "\n",
    "            k = (po - pe) / (1-pe)\n",
    "            kappas[key] = round(k, 2)\n",
    "\n",
    "        return kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KGLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_KG(self):\n",
    "        print(\"Loading knowledge graph from source\")\n",
    "        movie_graph = Graph()\n",
    "        movie_graph.parse(source='datasets/14_graph.nt', format='turtle')\n",
    "        print(\"succesfully loaded knowledge graph\")\n",
    "        return movie_graph\n",
    "\n",
    "    def load_embeddings(self, graph):\n",
    "        #add path parameter to retrive dictionaries?\n",
    "\n",
    "        DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "        RDFS = rdflib.namespace.RDFS\n",
    "        SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "\n",
    "        # load the embeddings\n",
    "        entity_emb = np.load(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'entity_embeds.npy'))\n",
    "        relation_emb = np.load(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'relation_embeds.npy'))\n",
    "\n",
    "        # load the dictionaries\n",
    "        with open(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'entity_ids.del'), 'r') as ifile:\n",
    "            ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "            id2ent = {v: k for k, v in ent2id.items()}\n",
    "        with open(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'relation_ids.del'), 'r') as ifile:\n",
    "            rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "            id2rel = {v: k for k, v in rel2id.items()}\n",
    "\n",
    "        ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
    "        lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
    "\n",
    "        embedding = (entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Returns a SQL statement as a string\n",
    "class QueryProducer:\n",
    "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "    WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "    def __init__(self, knowledgegraph):\n",
    "        self.moviegraph = knowledgegraph\n",
    "\n",
    "    def director_query(self, title):\n",
    "        query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:P57 ?director .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "        return query\n",
    "\n",
    "    # returns a dict, with all film titles and their entity Id\n",
    "    def all_films_id_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "        PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX p: <http://www.wikidata.org/prop/>\n",
    "        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?item ?label\n",
    "            WHERE{\n",
    "            ?item wdt:P31/wdt:P279* <http://www.wikidata.org/entity/Q11424> .\n",
    "            ?item rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = \"en\").\n",
    "        }\n",
    "                    '''\n",
    "        response = moviegraph.query(query)\n",
    "        id_to_film = {e[len(WD):]: str(i) for e,i in response}\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "        print(\"found normal films; \" + \"dict has size \" + str(len(film_to_id)))\n",
    "\n",
    "        sub_categories = self.subcategories_films_query()\n",
    "        print(\"found subcategories; \" + \"dict has size \" + str(len(sub_categories)))\n",
    "        sub_sub_categories = self.subcategories_of_subcategories_query()\n",
    "        print(\"found sub_subcategories; \" + \"dict has size \" + str(len(sub_sub_categories)))\n",
    "        animes = self.all_animes_id_query()\n",
    "        print(print(\"found animes; \" + \"dict has size \" + str(len(animes))))\n",
    "\n",
    "        film_to_id = {**film_to_id, **sub_categories, **sub_sub_categories, **animes}\n",
    "        print(\"the final film dict has size \" + str(len(film_to_id)))\n",
    "\n",
    "\n",
    "        return  film_to_id\n",
    "\n",
    "        # returns a dict, with all anime titles and their entity Id\n",
    "\n",
    "    def subcategories_films_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        # we iterate through the subcategories of the film category and get all subclasses\n",
    "        # then we iterate grough these subclasses and collect all movies\n",
    "        subcategory_query = '''\n",
    "            SELECT ?subcategory\n",
    "            WHERE {\n",
    "            ?subcategory ns1:P279 <http://www.wikidata.org/entity/Q11424> ;\n",
    "            }\n",
    "            '''\n",
    "        sub_response = self.moviegraph.query(subcategory_query)\n",
    "        subcategories = [str(_.subcategory) for _ in sub_response]\n",
    "\n",
    "        id_to_film = {}\n",
    "        for category in subcategories:\n",
    "            query = '''\n",
    "            SELECT ?item ?title\n",
    "            WHERE {\n",
    "            ?item ns1:P31 <''' + category + '''> .\n",
    "                  ?item rdfs:label ?title .\n",
    "            }\n",
    "            '''\n",
    "\n",
    "            response = self.moviegraph.query(query)\n",
    "            for e in response:\n",
    "                id_to_film[str(e.item)[len(WD):]] = str(e.title)\n",
    "\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "         # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "        return film_to_id\n",
    "\n",
    "    def subcategories_of_subcategories_query(self):\n",
    "        # AGAIN we iterate through the subcategories of the film category and get all\n",
    "        # But we also need to look at all the subcategories of the subcategory\n",
    "        # then we iterate grough these subclasses and collect all movies\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        subcategory_query = '''\n",
    "            SELECT ?subcategory\n",
    "            WHERE {\n",
    "            ?subcategory ns1:P279 <http://www.wikidata.org/entity/Q11424> ;\n",
    "            }\n",
    "            '''\n",
    "        sub_response = self.moviegraph.query(subcategory_query)\n",
    "        subcategories = [str(_.subcategory) for _ in sub_response]\n",
    "\n",
    "        print(\"len of subcategories: \" + str(len(subcategories)))\n",
    "\n",
    "\n",
    "        for category in subcategories:\n",
    "            sub_subcategory_query = '''\n",
    "                SELECT ?sub_subcategory\n",
    "                WHERE {\n",
    "                ?sub_subcategory ns1:P279 <''' + category + '''> ;\n",
    "                }\n",
    "            '''\n",
    "        sub_sub_response = self.moviegraph.query(sub_subcategory_query)\n",
    "        sub_subcategories = [str(_.sub_subcategory) for _ in sub_sub_response]\n",
    "\n",
    "        id_to_film = {}\n",
    "        for category in sub_subcategories:\n",
    "                query = '''\n",
    "            SELECT ?item ?title\n",
    "            WHERE {\n",
    "            ?item ns1:P31 <''' + category + '''> .\n",
    "                  ?item rdfs:label ?title .\n",
    "            }\n",
    "            '''\n",
    "\n",
    "                response = self.moviegraph.query(query)\n",
    "                for e in response:\n",
    "                    id_to_film[str(e.item)[len(WD):]] = str(e.title)\n",
    "\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "        return film_to_id\n",
    "\n",
    "    def all_animes_id_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "        PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX p: <http://www.wikidata.org/prop/>\n",
    "        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?item ?label\n",
    "            WHERE{\n",
    "            ?item wdt:P31 <http://www.wikidata.org/entity/Q20650540> .\n",
    "            ?item rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = \"en\").\n",
    "        }\n",
    "                    '''\n",
    "        response = self.moviegraph.query(query)\n",
    "        id_to_film = {e[len(WD):]: str(i) for e,i in response}\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "\n",
    "        return  film_to_id\n",
    "\n",
    "    def all_actors_id_and_IMDb_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        query ='''\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "            PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX p: <http://www.wikidata.org/prop/>\n",
    "            PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "            PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "            SELECT ?item ?label ?IMDb_ID\n",
    "                WHERE{\n",
    "                ?item wdt:P31 wd:Q5 .\n",
    "                ?item wdt:P106 wd:Q33999 .\n",
    "                ?item rdfs:label ?label .\n",
    "                ?item wdt:P345 ?IMDb_ID  .\n",
    "            }\n",
    "\n",
    "\n",
    "            '''\n",
    "\n",
    "        response = self.moviegraph.query(query)\n",
    "        ID_to_actor = {e[len(WD):]: str(i) for e,i, IDMb in response}\n",
    "        actor_to_Id = {value:key for key, value in ID_to_actor.items()}\n",
    "\n",
    "        ID_to_actor = {e[len(WD):]: str(IDMb) for e,i, IDMb in response}\n",
    "\n",
    "        return [actor_to_Id, ID_to_actor]\n",
    "\n",
    "    def predicateID_query(self, proposition):\n",
    "\n",
    "        query =\"\"\"\n",
    "\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        SELECT ?obj WHERE {\n",
    "            ?obj rdfs:label \"\"\" + '\"' + proposition + '\"' + \"\"\"@en .\n",
    "\n",
    "        }\n",
    "        \"\"\"\n",
    "        entity =self.moviegraph.query(query)\n",
    "        df = pd.DataFrame(entity.bindings)\n",
    "        url = str((df.iloc[0][0]))\n",
    "        id = url[len(QueryProducer.WDT):]\n",
    "\n",
    "        return id\n",
    "\n",
    "    def label_query(self, id):\n",
    "        query ='''\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "        SELECT  *\n",
    "        WHERE {\n",
    "            '''+id+''' rdfs:label ?label .\n",
    "            FILTER (langMatches( lang(?label), \"EN\" ) )\n",
    "          }\n",
    "\n",
    "                '''\n",
    "\n",
    "        response = moviegraph.query(query)\n",
    "        df = pd.DataFrame(response.bindings)\n",
    "        label = str((df.iloc[0][0]))\n",
    "        return label\n",
    "\n",
    "    def genre_id(self, title):\n",
    "        query =\"\"\"\n",
    "            prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "            SELECT ?obj WHERE {\n",
    "                ?item wdt:P31/wdt:P279* wd:Q201658 .\n",
    "                ?obj rdfs:label \"\"\" + '\"' + title + '\"' + \"\"\"@en .\n",
    "\n",
    "            }\n",
    "            \"\"\"\n",
    "        response = self.moviegraph.query(query)\n",
    "        response = [str(i[0]) for i in response]\n",
    "\n",
    "        return  response[0][len(QueryProducer.WD):]\n",
    "\n",
    "    def any_property_value_query(self, title, id, propertyname):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        \n",
    "        if \" \" in propertyname:\n",
    "            propertyname = propertyname.replace(\" \", \"_\")\n",
    "\n",
    "        query ='''\n",
    "                SELECT ?name\n",
    "                WHERE {\n",
    "                ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                      rdfs:label \"''' + title + '''\"@en ;\n",
    "                      ns1:'''+id+''' ?'''+propertyname+''' .\n",
    "                ?'''+propertyname+''' rdfs:label ?name\n",
    "                }\n",
    "                '''\n",
    "        return query\n",
    "\n",
    "    def any_property_value(self, title, id, propertyname):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        if \" \" in propertyname:\n",
    "            propertyname = propertyname.replace(\" \", \"_\")\n",
    "\n",
    "        query ='''\n",
    "                SELECT ?name\n",
    "                WHERE {\n",
    "                ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                      rdfs:label \"''' + title + '''\"@en ;\n",
    "                      ns1:'''+id+''' ?'''+ propertyname+''' .\n",
    "                ?'''+propertyname+''' rdfs:label ?name\n",
    "                }\n",
    "                '''\n",
    "        response = moviegraph.query(query)\n",
    "        df = pd.DataFrame(response.bindings)\n",
    "        try:\n",
    "            value = str((df.iloc[0][0]))\n",
    "        except:\n",
    "            value = \"nothing\"\n",
    "        return value\n",
    "\n",
    "    def all_genre_films(self, genre):\n",
    "        query ='''\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            SELECT  ?label\n",
    "            WHERE{\n",
    "              ?item wdt:P31/wdt:P279* wd:Q11424 .\n",
    "              ?item wdt:P136 wd:'''+genre+''' .\n",
    "              ?item rdfs:label ?label .\n",
    "              FILTER(LANG(?label) = \"en\").\n",
    "            } LIMIT 1000\n",
    "                '''\n",
    "        response = moviegraph.query(query)\n",
    "        labels = []\n",
    "        for e in response:\n",
    "            labels.append(str(e[0]))\n",
    "        df = pd.DataFrame(response.bindings, columns = [\"director\", \"subject\", \"period\"])\n",
    "        df['label'] = labels\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Recommander():\n",
    "    def __init__(self, moviegraph, queryproducer):\n",
    "        self.queryproducer = queryproducer\n",
    "        self.moviegraph = moviegraph\n",
    "\n",
    "    def decompose_question(self, question):\n",
    "        try:\n",
    "            if \",\" in question:\n",
    "                sub_sentences = question.split(\",\")\n",
    "\n",
    "            else:\n",
    "                sub_sentences = question.split(\"and\")\n",
    "        except:\n",
    "            return False\n",
    "        return sub_sentences\n",
    "\n",
    "    def find_simularities(self, movies):\n",
    "        genre_id = self.queryproducer.predicateID_query(\"genre\")\n",
    "        director_id = self.queryproducer.predicateID_query(\"director\")\n",
    "        main_subject_id = self.queryproducer.predicateID_query(\"main subject\")\n",
    "        set_in_period_id = self.queryproducer.predicateID_query(\"set in period\")\n",
    "\n",
    "        try:\n",
    "            genres = [self.queryproducer.any_property_value(movie, genre_id, \"genre\") for movie in movies]\n",
    "            directors = [self.queryproducer.any_property_value(movie, director_id, \"director\") for movie in movies]\n",
    "            main_subjects = [self.queryproducer.any_property_value(movie, main_subject_id, \"main subject\") for movie in movies]\n",
    "            set_in_periods = [self.queryproducer.any_property_value(movie, set_in_period_id, \"set in period\") for movie in movies]\n",
    "        except:\n",
    "            print(\"could not find properties of given movies\")\n",
    "            return False\n",
    "\n",
    "        properties = [genres, directors, main_subjects, set_in_periods]\n",
    "        for e in properties:\n",
    "            if \"nothing\" in e:\n",
    "                e.remove(\"nothing\")\n",
    "\n",
    "####### Collect the most common properties\n",
    "        genre = mode(genres)\n",
    "        director = mode(directors)\n",
    "        main_subject = mode(main_subjects)\n",
    "        set_in_period = mode(set_in_periods)\n",
    "        print(\"common genre: \" + str(genre))\n",
    "\n",
    "        try:\n",
    "            genre_value_id = self.queryproducer.genre_id(genre)\n",
    "        except:\n",
    "            print(\"could not find the id of the genre of recommander movies\")\n",
    "            return False\n",
    "##### Get all movies with the given genre, and some other properties\n",
    "        if genre:\n",
    "            try:\n",
    "                df = self.queryproducer.all_genre_films(genre_value_id)\n",
    "                print(\"found movies of same genre: \")\n",
    "                print(df.shape)\n",
    "            except:\n",
    "                print(\"could not find movie sof same genre\")\n",
    "                return False\n",
    "\n",
    "            for movie in movies:\n",
    "                try:\n",
    "                    df.drop(df[df[\"label\"] == movie].index, inplace = True)\n",
    "                except:\n",
    "                    print(\"could not delete instances of same movies\")\n",
    "\n",
    "            for label in df[\"label\"].values.tolist():\n",
    "                try:\n",
    "                    director_value = Q.any_property_value(label, director_id, \"director\")\n",
    "                    subject_value = Q.any_property_value(label, main_subject_id, \"main subject\")\n",
    "                    period_value = Q.any_property_value(label, set_in_period_id, \"set in period\")\n",
    "                    df.loc[df.label == label, \"director\"] = str(director_value)\n",
    "                    df.loc[df.label == label, \"subject\"] = str(subject_value)\n",
    "\n",
    "                    if period_value.endswith(\"s\"):\n",
    "                        period_value = period_value[:-1]\n",
    "                    df.loc[df.label == label, \"period\"] = str(period_value)\n",
    "                except:\n",
    "                    print(\"Could not add properties to dataframe\")\n",
    "                    pass\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "###### try to find a movies that share common properties\n",
    "        print(\"found common genre\")\n",
    "        common = {\"genre\": genre }\n",
    "\n",
    "        if not main_subject == \"nothing\":\n",
    "            df_subject = df.loc[df.subject == main_subject]\n",
    "            if not df_subject.empty:\n",
    "                print(\"found common subjects\")\n",
    "                common[\"main subject\"] = main_subject\n",
    "                df = df_subject\n",
    "\n",
    "        if not set_in_period == \"nothing\":\n",
    "            df_period = df.loc[df.period == set_in_period]\n",
    "            if not df_period.empty:\n",
    "                print(\"found common set in period\")\n",
    "                common[\"set in period\"] = set_in_period\n",
    "                df = df_period\n",
    "\n",
    "        if not director == \"nothing\":\n",
    "            df_director = df.loc[df.director == director]\n",
    "            if not df_director.empty:\n",
    "                print(\"found common director\")\n",
    "                common[\"director\"] = director\n",
    "                df = df_director\n",
    "\n",
    "        film_titles = df[\"label\"].tolist()\n",
    "        if len(film_titles) > 3:\n",
    "            film_titles = film_titles[:3]\n",
    "        common[\"film_titles\"]= film_titles\n",
    "\n",
    "        return common"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge graph from source\n",
      "succesfully loaded knowledge graph\n",
      "loaded KG and embeddings\n"
     ]
    }
   ],
   "source": [
    "KGloader = KGLoader()\n",
    "moviegraph = KGloader.load_KG()\n",
    "embedding = KGloader.load_embeddings(moviegraph)\n",
    "print(\"loaded KG and embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found normal films; dict has size 25509\n",
      "found subcategories; dict has size 2500\n",
      "len of subcategories: 59\n",
      "found sub_subcategories; dict has size 0\n",
      "found animes; dict has size 114\n",
      "None\n",
      "the final film dict has size 25509\n",
      "loaded film and actor dicts\n"
     ]
    }
   ],
   "source": [
    "Q = QueryProducer(moviegraph)\n",
    "actors_ID_dict = Q.all_actors_id_and_IMDb_query()[0]\n",
    "actor_IMDb_dict = Q.all_actors_id_and_IMDb_query()[1]\n",
    "all_films_dict = Q.all_films_id_query()\n",
    "print(\"loaded film and actor dicts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded image file\n"
     ]
    }
   ],
   "source": [
    "image_frame = pd.DataFrame.from_dict(json.load(open(\"images.json\")))\n",
    "print(\"loaded image file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupHandler:\n",
    "    def __init__(self, film_ids, actor_ids, actor_IMDb, query_producer, moviegraph, embedding, images, crowdsourcing):\n",
    "        self.film_ids = film_ids\n",
    "\n",
    "        print(\"titanic\" in self.film_ids.keys())\n",
    "\n",
    "        self.actor_IDs = actor_ids\n",
    "        self.actor_IMDb = actor_IMDb\n",
    "        self.query_producer = query_producer\n",
    "        self.movie_graph = moviegraph\n",
    "        self.embedding = embedding\n",
    "        self.images = images\n",
    "        self.crowdsourcing = crowdsourcing\n",
    "    \n",
    "    def film_id_finder(self, film_title):\n",
    "        # Film entity. Using lowercase title for lookup\n",
    "        try:\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            print(\"found film ID \" + str(film_id))\n",
    "            return film_id\n",
    "        except:\n",
    "            print(\"could not find film, trying to find it correcting typos...\")\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.remove_last_digit_space(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            print(\"found film ID \" + str(film_id))\n",
    "            return film_id\n",
    "        except:\n",
    "            print(\"could not find film, trying to find it correcting typos...\")\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.change_minus_sign(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            print(\"found film ID \" + str(film_id))\n",
    "            return film_id\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.change_dash_sign(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            print(\"found film ID \" + str(film_id))\n",
    "            return film_id\n",
    "        except:\n",
    "            print(\"could not find ID in id_dict of film: \"+ film_title)\n",
    "            raise KeyError(\"Film title contains typos or does not exist!\")\n",
    "\n",
    "    def graph_lookup(self, film_title, pred, pred_id):\n",
    "        \n",
    "        query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "        entity = self.movie_graph.query(query)\n",
    "        df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "        if df_entity.empty:\n",
    "            film_title = TypoCorrector.change_minus_sign(film_title)\n",
    "            query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "            entity = self.movie_graph.query(query)\n",
    "            df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "            if df_entity.empty:\n",
    "                film_title = TypoCorrector.change_dash_sign(film_title)\n",
    "                query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "                entity = self.movie_graph.query(query)\n",
    "                df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "                if df_entity.empty:\n",
    "                    film_title = TypoCorrector.remove_last_digit_space(film_title)\n",
    "                    query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "                    entity = self.movie_graph.query(query)\n",
    "                    df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "        # takes all answers but responds with the first one\n",
    "        property_value = []\n",
    "        for _ in entity:\n",
    "            property_value.append(str(_.name))\n",
    "        return property_value\n",
    "\n",
    "    def embedding_lookup(self, film_title, predicate, pred_id):\n",
    "        entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent = self.embedding\n",
    "\n",
    "        # define some prefixes\n",
    "        WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "\n",
    "        film_id = self.film_id_finder(film_title)\n",
    "        head = entity_emb[ent2id[WD[film_id]]]\n",
    "\n",
    "        pred = relation_emb[rel2id[WDT[pred_id]]]\n",
    "\n",
    "\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "        # compute ranks of entities\n",
    "        ranks = dist.argsort().argsort()\n",
    "\n",
    "        top_ten_results = pd.DataFrame([\n",
    "                (id2ent[idx][len(WD):], ent2lbl[id2ent[idx]], dist[idx], rank+1)\n",
    "                for rank, idx in enumerate(most_likely[:10])],\n",
    "                columns=('Entity', 'Label', 'Score', 'Rank'))\n",
    "        top_three = top_ten_results.iloc[0:3][\"Label\"]\n",
    "        top_three.values.tolist()\n",
    "\n",
    "        return top_three\n",
    "\n",
    "    def image_lookup(self, name, asFilmIdentifiedName):\n",
    "        try:\n",
    "            if name.endswith(\" \"):\n",
    "                name = name[:-1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if asFilmIdentifiedName.endswith(\" \"):\n",
    "                name = asFilmIdentifiedName[:-1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            actor_Id = self.actor_IDs[name]\n",
    "        except:\n",
    "            actor_Id = self.actor_IDs[asFilmIdentifiedName]\n",
    "\n",
    "        IMDb_Id = self.actor_IMDb[actor_Id]\n",
    "        valid_options = self.images.loc[self.images[\"cast\"].str.contains(IMDb_Id, regex=False)]\n",
    "    \n",
    "        #only images where actor appears alone and its a poster. Take a random sample\n",
    "        try:\n",
    "            valid_options = valid_options.loc[valid_options[\"type\"]==\"poster\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            valid_options = valid_options.query(\"cast.str.len() == 1\", engine=\"python\")\n",
    "        except:\n",
    "            pass\n",
    "        valid_options = valid_options[\"img\"]\n",
    "        valid_options = valid_options.sample()\n",
    "\n",
    "        url = str(valid_options.iloc[0])\n",
    "        return \"image:\"+url[:-4]\n",
    "\n",
    "    # Receives and returns a dict with batch IDm batch kappa, the verdict,  agrrement nr, disagreement nr, and the value\n",
    "    def crowdsourcing_lookup(self, filmtitle, pred_id):\n",
    "        film_id = self.film_id_finder(filmtitle)\n",
    "        answer = self.crowdsourcing.crowdsourcing_answer(film_id, pred_id)\n",
    "        print(\"Value is: \" + answer[\"value\"])\n",
    "        if answer[\"value\"].startswith(\"wd:\"):\n",
    "            label = self.query_producer.label_query(answer[\"value\"])\n",
    "            answer[\"value\"] =  label\n",
    "            return answer\n",
    "        if answer[\"value\"].startswith(\"Q\"):\n",
    "            label = self.query_producer.label_query(\"wd:\"+answer[\"value\"])\n",
    "            answer[\"value\"] =  label\n",
    "            return answer\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Takes an argument \"training\" to determine if it should create and save a model or load an existing one\n",
    "class RunProcess:\n",
    "    def __init__(self, training, knowledgegraph):\n",
    "        print(\"Starting process\")\n",
    "        self.ner_model = NerTrainer()\n",
    "        if training:\n",
    "            self.crf_model = self.ner_model.train()\n",
    "            print(\"Created and trained model succesfully\")\n",
    "            with open('crf_model.pkl', 'wb') as f:\n",
    "                pickle.dump(self.crf_model, f)\n",
    "            print(\"Saved crf-model\")\n",
    "        else:\n",
    "            #self.ner_model = NerDLModel.pretrained('ner_mit_movie_simple_distilbert_base_cased', 'en')\\\n",
    "            #.setInputCols(['document', 'token', 'embeddings']).setOutputCol('ner')\n",
    "            with open('crf_model.pkl', 'rb') as f:\n",
    "                self.crf_model = pickle.load(f)\n",
    "            print(\"loaded ner-mnodel\")\n",
    "\n",
    "        self.movie_graph = knowledgegraph\n",
    "        self.embedding = embedding\n",
    "        self.actor_IDs = actors_ID_dict\n",
    "        self.actor_IMDb = actor_IMDb_dict\n",
    "        self.film_ids = all_films_dict #combine both to one dict\n",
    "        self.images = image_frame\n",
    "\n",
    "        self.crowdourcing = Crowdsourcing()\n",
    "        self.question_analyser = QuestionAnalyser()\n",
    "        self.query_producer = QueryProducer(self.movie_graph)\n",
    "        self.lookup_handler = LookupHandler(self.film_ids, self.actor_IDs, self.actor_IMDb,\n",
    "            self.query_producer, self.movie_graph, self.embedding, self.images, self.crowdourcing)\n",
    "        self.recommander = Recommander(self.movie_graph, self.query_producer)\n",
    "\n",
    "        print(\"----------Run Initialized!----------\")\n",
    "\n",
    "    def answer_movie_question(self, *args):\n",
    "        if not args:\n",
    "            question = self.ask_question()\n",
    "        else:\n",
    "            print(args)\n",
    "            question = args[0][\"message\"]\n",
    "        film_title, actor, pred = self.identify_subject_and_predicate(question)\n",
    "        print(\"predicate found and handed to answer_movie_question: \")\n",
    "        print(pred)\n",
    "\n",
    "        if pred == \"recommendation\":\n",
    "            if question[-1] in [\".\", \"?\", \"!\"]:\n",
    "                question = question[:-1]\n",
    "            movies = self.recommander.decompose_question(question= question)\n",
    "            if not movies:\n",
    "                answer = [\"Fail1\"]\n",
    "                answer = self.answer_designer(film_title=\"\", answer=answer, pred=\"recommendation\")\n",
    "                return answer\n",
    "            else:\n",
    "                subjects = []\n",
    "\n",
    "                for movie in movies:\n",
    "                    tagged_entities = self.ner_model.predict_tags(self.crf_model, movie)\n",
    "                    film_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-TITLE','I-TITLE'])]\n",
    "                    film_title = ''\n",
    "                    for w in film_wordlist:\n",
    "                        film_title = film_title + w + ' '\n",
    "                    if film_title.endswith(\" \"):\n",
    "                        film_title = film_title[:-1]\n",
    "                    subjects.append(film_title)\n",
    "\n",
    "                answer = self.recommander.find_simularities(subjects)\n",
    "                if not answer:\n",
    "                    answer = [\"Fail2\"]\n",
    "                    answer = self.answer_designer(film_title=\"\", answer=answer, pred=\"recommendation\")\n",
    "                else:\n",
    "                    answer = self.answer_designer(film_title=\"\", answer=answer, pred=\"recommendation\")\n",
    "            return answer\n",
    "\n",
    "        #set pred to defualt value\n",
    "        if not pred:\n",
    "            pred = \"director\"\n",
    "        # some predicates cannot be fetched by the query. Their\n",
    "        if pred in self.question_analyser.exceptional_predicates.keys():\n",
    "            pred_id = self.question_analyser.handle_exceptional_predicates(pred)\n",
    "        elif pred[:-1] in self.question_analyser.exceptional_predicates.keys():\n",
    "            pred_id = self.question_analyser.handle_exceptional_predicates(pred[:-1])\n",
    "        else:\n",
    "            pred_id = self.query_producer.predicateID_query(pred)\n",
    "\n",
    "        film_title, actor = TypoCorrector.title_corrector(film_title, actor)\n",
    "\n",
    "        print(\"subject: \"+ str(film_title) + \" | actor: \" +str(actor)+ \" | predicate: \"+str(pred)+\" | predicate ID: \"+str(pred_id) )\n",
    "        print(\"##################################################################\")\n",
    "\n",
    "        answer_set = self.lookup(actor=actor, film_title=film_title, pred=pred, pred_id=pred_id)\n",
    "\n",
    "        answer = self.answer_designer(film_title=film_title, pred=pred, answer=answer_set)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def ask_question(self):\n",
    "        question = input(\"Please ask your question\")\n",
    "        return question\n",
    "\n",
    "    def identify_subject_and_predicate(self, question):\n",
    "        tagged_entities = self.ner_model.predict_tags(self.crf_model, question)\n",
    "        print(\"Sentence with tags: \")\n",
    "        print(tagged_entities)\n",
    "        print(\"#################################################################\")\n",
    "        identified_predicate = self.question_analyser.identify_predicate(tagged_entities)\n",
    "\n",
    "        if not identified_predicate:\n",
    "            recommender = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\"]\n",
    "            if any(c in tagged_entities[\"WORD\"].unique() for c in recommender):\n",
    "                return 0, 0, \"recommendation\"\n",
    "\n",
    "\n",
    "        if \"B-TITLE\" not in tagged_entities[\"TAG\"].unique():\n",
    "            print(\"changing Tags because no title tags found\")\n",
    "            try:\n",
    "                idx = tagged_entities.index[tagged_entities[\"TAG\"]==\"I-PLOT\"].tolist()\n",
    "                tagged_entities.iloc[idx[0]:idx[0]+1][\"TAG\"] = 'B-TITLE'\n",
    "                tagged_entities.loc[tagged_entities['TAG'] == \"I-PLOT\", 'TAG'] = 'I-TITLE'\n",
    "            except:\n",
    "                print(\"could not add Title Tags\")\n",
    "\n",
    "\n",
    "        #TODO Here we can also check for actors. ALSO we can check if I-title is a questions mark, comma etc.\n",
    "        film_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-TITLE','I-TITLE'])]\n",
    "        film_title = ''\n",
    "        for w in film_wordlist:\n",
    "            film_title = film_title + w + ' '\n",
    "\n",
    "        actor_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-ACTOR','I-ACTOR'])]\n",
    "        actor = ''\n",
    "        for w in actor_wordlist:\n",
    "            actor = actor + w + ' '\n",
    "        # need to remove empry string at end of concatenation\n",
    "        film_title = film_title[:-1]\n",
    "        actor = actor[:-1]\n",
    "\n",
    "        return film_title, actor, identified_predicate\n",
    "\n",
    "\n",
    "        film_id = self.film_id_finder(filmtitle)\n",
    "        answer = self.crowdourcing.crowdsourcing_answer(film_id, pred_id)\n",
    "        return answer\n",
    "\n",
    "    def lookup(self, actor, film_title, pred, pred_id):\n",
    "        answer = {}\n",
    "        print(\"doing lookup for: \" + str(actor) +\" |\"+ str(film_title) + \" |\" + str(pred))\n",
    "\n",
    "        if pred == 'image':\n",
    "            try:\n",
    "                image_url = self.lookup_handler.image_lookup(name = actor, asFilmIdentifiedName=film_title)\n",
    "                print(\"Image of \" + film_title + actor + \" at this url: \"+ image_url)\n",
    "                answer[\"image\"] = image_url\n",
    "            except:\n",
    "                answer[\"image\"] = \"Unfortunately I was unable to find an image of:\" + film_title + actor\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                answer_crowdsourcing = self.lookup_handler.crowdsourcing_lookup(film_title, pred_id)\n",
    "                answer[\"crowdsourcing\"] = answer_crowdsourcing\n",
    "                print(\"found crowdsourcing answer\")\n",
    "            except:\n",
    "                print(\"Could not find crowd answer\")\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                # this answer might have multiple answers in a list\n",
    "                answer_graph = self.lookup_handler.graph_lookup(film_title, pred, pred_id)\n",
    "                answer[\"graph\"] = answer_graph[0]\n",
    "                print(\"found KG answer\")\n",
    "            except:\n",
    "                print(\"Could not find graph answer\")\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                answer_embedding = self.lookup_handler.embedding_lookup(film_title, pred, pred_id)\n",
    "                answer[\"embedding\"] = answer_embedding\n",
    "                print(\"found embedding answer\")\n",
    "            except:\n",
    "                print(\"Could not embedding crowd answer\")\n",
    "                pass\n",
    "        return answer\n",
    "\n",
    "    def answer_designer(self, film_title, pred, answer):\n",
    "        answer_sentence = \"\"\n",
    "\n",
    "        if pred == \"recommendation\":\n",
    "            if answer == \"Fail1\":\n",
    "                return \"I'm sorry, but I could not properly identify the movies you gave me, hence I cant give you any recommendation.\"\n",
    "            if answer == \"Fail2\":\n",
    "                return \"I'm sorry, but I could not find any proper recommendation that matches the movies you gave me!\"\n",
    "            else:\n",
    "                answer_sentence = \"According to the movies you gave me, I was looking for movies of the genre: \" +answer[\"genre\"]\n",
    "                if \"main subject\" in answer.keys():\n",
    "                    answer_sentence + \", the mains subject \" + answer[\"main subject\"]\n",
    "                if \"set in period\" in answer.keys():\n",
    "                    answer_sentence + \", where the set in period is \" + answer[\"set in period\"]\n",
    "                if \"director\" in answer.keys():\n",
    "                    answer_sentence + \" and the director is \" + answer[\"director\"] + \".\"\n",
    "                if len(answer[\"film_titles\"]) == 0:\n",
    "                    answer_sentence + \"Unfortunately I was not able to find any movies matching at least one criteria.\"\n",
    "                else:\n",
    "                    films = \". Those are the film that I can recommend you: \"\n",
    "                    for film in answer[\"film_titles\"]:\n",
    "                        films = films + film + \", \"\n",
    "                    answer_sentence = answer_sentence + films[:-1] +\".\"\n",
    "                return answer_sentence\n",
    "\n",
    "\n",
    "        if not answer:\n",
    "            answer_sentence = \"I'm sorry but I could not find an answer to your question. Double check for typos or try to ask a different question\"\n",
    "        else:\n",
    "            # answer is a dict with keys: image, crowdsourcing, graph, embedding\n",
    "            keys = answer.keys()\n",
    "            # if we need to return an image, its easy\n",
    "            if \"image\" in keys:\n",
    "                return answer[\"image\"]\n",
    "\n",
    "            # Then we check for the other possible answers\n",
    "            # Keep in mind that the answers might consist of iterals\n",
    "            # We will setup the answer as a combination of multiple\n",
    "\n",
    "            # First we check if the knowledgegraph gave us something and if yes, if the embedding can React to it.\n",
    "            if \"graph\" in keys:\n",
    "                answer_sentence = answer_sentence + \"According to the knowledge-graph, the \" + str(pred) + \" of \" + str(film_title) + \" is \" + str(answer[\"graph\"]) + \".\"\n",
    "                if \"embedding\" in keys:\n",
    "                    if answer[\"graph\"] == answer[\"embedding\"][0]:\n",
    "                        print(\"graph - embeddding case 1\")\n",
    "                        answer_sentence = answer_sentence + \" The top answer by the embedding is the same, while having \" + str(answer[\"embedding\"][1]) + \" as second most likely answer.\"\n",
    "                    elif answer[\"graph\"] == answer[\"embedding\"][1]:\n",
    "                        print(\"graph - embeddding case 2\")\n",
    "                        answer_sentence = answer_sentence + \" The embedding actually suggests \" + str(answer[\"embedding\"][0]) +\" as most likely answer, while \" + str(answer[\"embedding\"][1]) + \" only comes second.\"\n",
    "                    elif answer[\"graph\"] not in  answer[\"embedding\"]:\n",
    "                        print(\"graph - embeddding case 3\")\n",
    "                        answer_sentence = answer_sentence + \" The embedding actually suggests either \" + str(answer[\"embedding\"][0]) +\" or \" + str(answer[\"embedding\"][1]) + \" as more likely answers though.\"\n",
    "\n",
    "            # Since there was no answer by the knowledge-graph, we have to rephrase the sentences\n",
    "            else:\n",
    "                if \"embedding\" in keys:\n",
    "                    answer_sentence = answer_sentence +  \"The embedding suggests that \" + str(answer[\"embedding\"][0]) +\" is the most likely answer and \" + str(answer[\"embedding\"][1]) + \" as the second most likely answers.\"\n",
    "\n",
    "            if \"crowdsourcing\" in keys:\n",
    "                batch_id = answer[\"crowdsourcing\"][\"batch\"]\n",
    "                kappa = str(answer[\"crowdsourcing\"][\"kappa\"])\n",
    "                verdict = answer[\"crowdsourcing\"][\"verdict\"]\n",
    "                agreement = answer[\"crowdsourcing\"][\"agreement\"]\n",
    "                disagreement = answer[\"crowdsourcing\"][\"disagreement\"]\n",
    "                value = str(answer[\"crowdsourcing\"][\"value\"])\n",
    "\n",
    "                if verdict ==\"True\":\n",
    "                    answer_sentence = answer_sentence + \" When concerning crowdsourcing, the crowd agrees that \" + value + \" is the \" + str(pred) + \" of \" + str(film_title)[:-1]+ \". \" + str(agreement) +\" vote in favour of this statement and \"+ str(disagreement) + \" vote against it. The interrater agreement of this batch is \"+kappa+\".\"\n",
    "\n",
    "                elif verdict ==\"False\":\n",
    "                    answer_sentence = answer_sentence + \" When concerning crowdsourcing, the crowd diagrees that \" + value + \" is the \" + str(pred) + \" of \" + str(film_title)[:-1]+ \". \"+ str(agreement) +\" vote in favour of this statement and \"+ str(disagreement) + \" vote against it. The interrater agreement of this batch is \"+kappa+\".\"\n",
    "\n",
    "                elif verdict ==\"corrected\":\n",
    "                    answer_sentence = answer_sentence + \" When concerning crowdsourcing, the crowd disagreed on the proposed statement, but suggests that that \" + value + \" is the actual \" + str(pred) + \" of \" + str(film_title)+ \". \" +\"The interrater agreement of this batch is \"+kappa+\".\"\n",
    "\n",
    "                elif verdict==\"tie\":\n",
    "                    answer_sentence = answer_sentence + \" When concerning crowdsourcing, the crowd is undecided if \" + value + \" is the \" + str(pred) + \" of \" + str(film_title)[:-1]+ \". \"+ str(agreement) +\" vote in favour of this statement and \"+ str(disagreement) + \" vote against it. The interrater agreement of this batch is \"+kappa +\".\"\n",
    "\n",
    "        return answer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# url of the speakeasy server\n",
    "url = 'https://speakeasy.ifi.uzh.ch'\n",
    "listen_freq = 3\n",
    "\n",
    "\n",
    "class DemoBot:\n",
    "    def __init__(self, username, password):\n",
    "        self.agent_details = self.login(username, password)\n",
    "        self.session_token = self.agent_details['sessionToken']\n",
    "        self.chat_state = defaultdict(lambda: {'messages': defaultdict(dict), 'initiated': False, 'my_alias': None})\n",
    "        self.process = RunProcess(training=False, knowledgegraph=moviegraph)\n",
    "\n",
    "        atexit.register(self.logout)\n",
    "\n",
    "    def listen(self):\n",
    "        while True:\n",
    "            # check for all chatrooms\n",
    "            current_rooms = self.check_rooms(session_token=self.session_token)['rooms']\n",
    "            for room in current_rooms:\n",
    "                # ignore finished conversations\n",
    "                if room['remainingTime'] > 0:\n",
    "                    room_id = room['uid']\n",
    "                    if not self.chat_state[room_id]['initiated']:\n",
    "                        # send a welcome message and get the alias of the agent in the chatroom\n",
    "                        self.post_message(room_id=room_id, session_token=self.session_token, message='Hi, you can send me any message and check if it is echoed in {} seconds.'.format(listen_freq))\n",
    "                        self.chat_state[room_id]['initiated'] = True\n",
    "                        self.chat_state[room_id]['my_alias'] = room['alias']\n",
    "\n",
    "                    # check for all messages\n",
    "                    all_messages = self.check_room_state(room_id=room_id, since=0, session_token=self.session_token)['messages']\n",
    "\n",
    "                    # you can also use [\"reactions\"] to get the reactions of the messages: STAR, THUMBS_UP, THUMBS_DOWN\n",
    "\n",
    "                    for message in all_messages:\n",
    "                        if message['authorAlias'] != self.chat_state[room_id]['my_alias']:\n",
    "\n",
    "                            # check if the message is new\n",
    "                            if message['ordinal'] not in self.chat_state[room_id]['messages']:\n",
    "                                self.chat_state[room_id]['messages'][message['ordinal']] = message\n",
    "                                print('\\t- Chatroom {} - new message #{}: \\'{}\\' - {}'.format(room_id, message['ordinal'], message['message'], self.get_time()))\n",
    "\n",
    "                                try:\n",
    "                                    answer = self.process.answer_movie_question(message)\n",
    "                                except:\n",
    "                                    answer = \"Sorry I coult not answer your question, please try something else.\"\n",
    "\n",
    "                                #self.post_message(room_id=room_id, session_token=self.session_token, message='Got your message: \\'{}\\' at {}.'.format(message['message'], self.get_time()))\n",
    "                                self.post_message(room_id=room_id, session_token=self.session_token, message=answer)\n",
    "            time.sleep(listen_freq)\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        agent_details = requests.post(url=url + \"/api/login\", json={\"username\": username, \"password\": password}).json()\n",
    "        print('- User {} successfully logged in with session \\'{}\\'!'.format(agent_details['userDetails']['username'], agent_details['sessionToken']))\n",
    "        return agent_details\n",
    "\n",
    "    def check_rooms(self, session_token: str):\n",
    "        return requests.get(url=url + \"/api/rooms\", params={\"session\": session_token}).json()\n",
    "\n",
    "    def check_room_state(self, room_id: str, since: int, session_token: str):\n",
    "        return requests.get(url=url + \"/api/room/{}/{}\".format(room_id, since), params={\"roomId\": room_id, \"since\": since, \"session\": session_token}).json()\n",
    "\n",
    "    def post_message(self, room_id: str, session_token: str, message: str):\n",
    "        tmp_des = requests.post(url=url + \"/api/room/{}\".format(room_id),\n",
    "                                params={\"roomId\": room_id, \"session\": session_token}, data=message).json()\n",
    "        if tmp_des['description'] != 'Message received':\n",
    "            print('\\t\\t Error: failed to post message: {}'.format(message))\n",
    "\n",
    "    def get_time(self):\n",
    "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n",
    "\n",
    "    def logout(self):\n",
    "        if requests.get(url=url + \"/api/logout\", params={\"session\": self.session_token}).json()['description'] == 'Logged out':\n",
    "            print('- Session \\'{}\\' successfully logged out!'.format(self.session_token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Chatroom 3c24343c-065a-498d-b8af-fe931b3ab37d - new message #33: 'What was the filming location of Titanic?' - 21:37:54, 12-12-2022\n",
      "({'timeStamp': 1670877472170, 'authorAlias': 'awesome_greider_2', 'ordinal': 33, 'message': 'What was the filming location of Titanic?'},)\n",
      "Sentence with tags: \n",
      "       WORD  POS      TAG\n",
      "0      What   WP        0\n",
      "1       was  VBD        0\n",
      "2       the   DT        0\n",
      "3   filming   JJ        0\n",
      "4  location   NN        0\n",
      "5        of   IN        0\n",
      "6   Titanic  NNP  B-TITLE\n",
      "7         ?    .  I-TITLE\n",
      "#################################################################\n",
      "['filming', 'location']\n",
      "['wa']\n",
      "max valu in identity counter: \n",
      "filming location 1\n",
      "pred to return:\n",
      "filming location\n",
      "predicate found and handed to answer_movie_question: \n",
      "filming location\n",
      "subject: Titanic  | actor:  | predicate: filming location | predicate ID: P915\n",
      "##################################################################\n",
      "doing lookup for:  |Titanic  |filming location\n",
      "could not find film, trying to find it correcting typos...\n",
      "found film ID Q1426605\n",
      "Could not find crowd answer\n",
      "found KG answer\n",
      "could not find film, trying to find it correcting typos...\n",
      "found film ID Q1426605\n",
      "found embedding answer\n",
      "graph - embeddding case 3\n",
      "\t- Chatroom b343eb90-21f2-4c9c-987f-bb44670cc424 - new message #1: 'Who is the executive producer of The Notebook?' - 21:39:16, 12-12-2022\n",
      "({'timeStamp': 1670877552204, 'authorAlias': 'kind_kilby_2', 'ordinal': 1, 'message': 'Who is the executive producer of The Notebook?'},)\n",
      "Sentence with tags: \n",
      "        WORD  POS      TAG\n",
      "0        Who   WP        0\n",
      "1         is  VBZ        0\n",
      "2        the   DT        0\n",
      "3  executive   JJ        0\n",
      "4   producer   NN        0\n",
      "5         of   IN        0\n",
      "6        The   DT  B-TITLE\n",
      "7   Notebook  NNP  I-TITLE\n",
      "8          ?    .  I-TITLE\n",
      "#################################################################\n",
      "['executive', 'producer']\n",
      "['is']\n",
      "max valu in identity counter: \n",
      "executive producer 2\n",
      "pred to return:\n",
      "executive producer\n",
      "predicate found and handed to answer_movie_question: \n",
      "executive producer\n",
      "subject: The Notebook  | actor:  | predicate: executive producer | predicate ID: P1431\n",
      "##################################################################\n",
      "doing lookup for:  |The Notebook  |executive producer\n",
      "could not find film, trying to find it correcting typos...\n",
      "found film ID Q223374\n",
      "Could not find crowd answer\n",
      "found KG answer\n",
      "could not find film, trying to find it correcting typos...\n",
      "found film ID Q223374\n",
      "found embedding answer\n",
      "graph - embeddding case 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-68-cb74effaea51>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m#password = getpass.getpass('Password of the demo bot:')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mdemobot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDemoBot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0musername\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"6dNtSBEkgDAl0g\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mdemobot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlisten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-67-ff5a60cc9540>\u001B[0m in \u001B[0;36mlisten\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     47\u001B[0m                                 \u001B[1;31m#self.post_message(room_id=room_id, session_token=self.session_token, message='Got your message: \\'{}\\' at {}.'.format(message['message'], self.get_time()))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m                                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpost_message\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroom_id\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mroom_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msession_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msession_token\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlisten_freq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mlogin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0musername\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpassword\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "username = 'dean.heizmann_bot'\n",
    "#password = getpass.getpass('Password of the demo bot:')\n",
    "demobot = DemoBot(username, \"6dNtSBEkgDAl0g\")\n",
    "demobot.listen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Process3 = RunProcess(training=False, knowledgegraph=moviegraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(Process3.answer_movie_question())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nRecommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\\nRecommend movies like X-Men: First Class, The Notebook, and Titanic.\\n\"Titanic\", \"The Notebook\", \"X-Men: First Class?\"\\nWho is the director of Good Will Hunting?\\n\\nWho produced The Lion King?\\nTell me the publication date of The Lion King.\\n\\nWhat is the Kinopoisk film ID of The Notebook?\\nTell me the exploitation visa number of X-Men: First Class?\\n\\nWho is the screenwriter of The Masked Gang: Cyprus\\nWho is the screenwriter of Titanic?\\nWho is the screenwriter of Weathering with You\\n\\nWho is the director of Star Wars: Episode VI - Return of the Jedi?\\nWho is the executive producer of X-Men: First Class?\\nWhat is the genre of The Lion King?\\nWhat is the MPA film rating of The Lion King?\\nWhat is the MPA film rating of The Notebook?\\n\\nWhat is the MPAA film rating of Weathering with You?\\nWhat is the MPAA film rating of Titanic?\\nWhat is the box office of The Princess and the Frog?\\nCan you tell me the publication date of Tom Meets Zizou?\\n\\nShow me a picture of Julia Roberts.\\n\\nThe Lion King\\nThe Notebook\\nTitanic\\n'"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tell me the publication date of Spy Game\n",
    "Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\n",
    "Recommend movies like X-Men: First Class, The Notebook, and Titanic.\n",
    "\"Titanic\", \"The Notebook\", \"X-Men: First Class?\"\n",
    "Who is the director of Good Will Hunting?\n",
    "\n",
    "Who produced The Lion King?\n",
    "Tell me the publication date of The Lion King.\n",
    "\n",
    "What is the Kinopoisk film ID of The Notebook?\n",
    "Tell me the exploitation visa number of X-Men: First Class?\n",
    "\n",
    "Who is the screenwriter of The Masked Gang: Cyprus\n",
    "Who is the screenwriter of Titanic?\n",
    "Who is the screenwriter of Weathering with You\n",
    "\n",
    "Who is the director of Star Wars: Episode VI - Return of the Jedi?\n",
    "Who is the executive producer of X-Men: First Class?\n",
    "What is the genre of The Lion King?\n",
    "What is the MPA film rating of The Lion King?\n",
    "What is the MPA film rating of The Notebook?\n",
    "\n",
    "What is the MPAA film rating of Weathering with You?\n",
    "What is the MPAA film rating of Titanic?\n",
    "What is the box office of The Princess and the Frog?\n",
    "Can you tell me the publication date of Tom Meets Zizou?\n",
    "\n",
    "Show me a picture of Julia Roberts.\n",
    "\n",
    "The Lion King\n",
    "The Notebook\n",
    "Titanic\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"Tell me the publication data of Spy Game\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action film\n"
     ]
    }
   ],
   "source": [
    "Q = QueryProducer(knowledgegraph=moviegraph)\n",
    "\n",
    "value = Q.any_property_value(\"Spy Game\", \"P2047\", \"duration\")\n",
    "\n",
    "print(value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{rdflib.term.Variable('value'): rdflib.term.URIRef('http://www.wikidata.org/entity/Q188473')}, {rdflib.term.Variable('value'): rdflib.term.URIRef('http://www.wikidata.org/entity/Q21401869')}, {rdflib.term.Variable('value'): rdflib.term.URIRef('http://www.wikidata.org/entity/Q2297927')}, {rdflib.term.Variable('value'): rdflib.term.URIRef('http://www.wikidata.org/entity/Q3990883')}, {rdflib.term.Variable('value'): rdflib.term.URIRef('http://www.wikidata.org/entity/Q622291')}]\n"
     ]
    }
   ],
   "source": [
    "query = ''' prefix wd: <http://www.wikidata.org/entity/>\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "        SELECT ?value\n",
    "        WHERE {\n",
    "            wd:Q1049139 wdt:P136 ?value .\n",
    "        }\n",
    "'''\n",
    "\n",
    "entity = moviegraph.query(query)\n",
    "print(entity.bindings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "949167c31c494cbe6f2153fb142c4e45576b141632160fdf2c3b2b305db6da2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}