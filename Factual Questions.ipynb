{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from rdflib import Graph, URIRef\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import json\n",
    "import time\n",
    "import atexit\n",
    "import getpass\n",
    "import requests  # install the package via \"pip install requests\"\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "import rdflib\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#from sparknlp.annotator import *\n",
    "#from sparknlp.base import *\n",
    "#import sparknlp\n",
    "#import spark\n",
    "import urllib.request\n",
    "\n",
    "#from sparknlp.annotator import *\n",
    "#from sparknlp.base import *\n",
    "#import sparknlp\n",
    "#spark = sparknlp.start()\n",
    "\n",
    "#from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "###\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Answer factual questions\n",
    "Model training need to be applied only once. The function is at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NerTrainer:\n",
    "    #Most of the funcitons are from the Graphical Models Tutorial from the class\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_train = pd.read_csv('engtrain_clean.csv', encoding=\"ISO-8859-1\")\n",
    "        self.x_test = pd.read_csv('engtest_clean.csv', encoding=\"ISO-8859-1\")\n",
    "        print(\"Loaded data & initialized NER-model\")\n",
    "\n",
    "    def collate(self, dataframe):\n",
    "        agg_func = lambda s: [(w, pos, t) for w, pos, t in\n",
    "                              zip(s['WORD'].values.tolist(), s['POS'].values.tolist(), s['ï»¿TAG'].values.tolist())]\n",
    "        grouped = dataframe.groupby('SENTENCE').apply(agg_func)\n",
    "        return list(grouped)\n",
    "\n",
    "    # counts sentences in given input data\n",
    "    def count_sentences(self, data):\n",
    "        counter = 0\n",
    "        for index, row in data.iterrows():\n",
    "            data.at[index, 'SENTENCE'] = counter\n",
    "            if pd.isna(data.iloc[index, 0]):\n",
    "                counter += 1\n",
    "        data[\"POS\"] = \"0\"\n",
    "        data = data.dropna()\n",
    "        data = data.reset_index(drop=True)\n",
    "        return data\n",
    "\n",
    "    def word2features(self, sent, i):\n",
    "        word = sent[i][0]\n",
    "        postag = sent[i][1]\n",
    "\n",
    "        features = {\n",
    "            'word.lower()': word.lower(),  # the word in lowercase\n",
    "            'word[-3:]': word[-3:],  # last three characters\n",
    "            'word[-2:]': word[-2:],  # last two characters\n",
    "            'word.isupper()': word.isupper(),  # true, if the word is in uppercase\n",
    "            'word.istitle()': word.istitle(),\n",
    "            # true, if the first character is in uppercase and remaining characters are in lowercase\n",
    "            'word.isdigit()': word.isdigit(),  # true, if all characters are digits\n",
    "            'postag': postag,  # POS tag\n",
    "            'postag[:2]': postag[:2],  # IOB prefix\n",
    "        }\n",
    "\n",
    "        if i > 0:\n",
    "            word1 = sent[i - 1][0]  # the previous word\n",
    "            postag1 = sent[i - 1][1]  # POS tag of the previous word\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2],\n",
    "            })  # add some features of the previous word\n",
    "        else:\n",
    "            features['BOS'] = True  # BOS: begining of the sentence\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i + 1][0]  # the next word\n",
    "            postag1 = sent[i + 1][1]  # POS tag of the next word\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2],\n",
    "            })  # add some features of the next word\n",
    "        else:\n",
    "            features['EOS'] = True  # EOS: end of the sentence\n",
    "        return features\n",
    "\n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for _, _, label in sent]\n",
    "\n",
    "    def tag_data(self, data_to_tag):\n",
    "        tags = []\n",
    "        x_train = data_to_tag.groupby(['SENTENCE'])\n",
    "        x_train = x_train.apply(np.mean)\n",
    "\n",
    "        # x_train.shape[0] will give us the number of sentences to loop through and tag them using nltk lib\n",
    "        # first we locate the sentence in our input data, then we hand the sentence as a list of words to the nltk function which tags the words.\n",
    "        for i in range(x_train.shape[0]):\n",
    "            sentence = data_to_tag.loc[data_to_tag[\"SENTENCE\"] == i]\n",
    "            sentence = sentence['WORD'].tolist()\n",
    "            tag = nltk.pos_tag(sentence)\n",
    "            tags.append(tag)\n",
    "\n",
    "        #now we add the found tags to the actual data in the data set\n",
    "        counter = 0\n",
    "        for sentence in tags:\n",
    "            for word in sentence:\n",
    "                data_to_tag.at[counter, 'POS'] = word[1]\n",
    "                counter += 1\n",
    "        print(\"data to tag in \"\"tag_data()\"\":\" )\n",
    "        print(data_to_tag)\n",
    "        print(\"#######################################################\")\n",
    "        return data_to_tag\n",
    "\n",
    "    #code copied from graphical_model tutorial and collected under one train function\n",
    "    def train(self):\n",
    "        self.x_train = self.count_sentences(self.x_train)\n",
    "        self.x_test = self.count_sentences(self.x_test)\n",
    "\n",
    "        self.x_train = self.tag_data(self.x_train)\n",
    "        self.x_test = self.tag_data((self.x_test))\n",
    "\n",
    "        self.x_train = self.collate(self.x_train)\n",
    "        self.x_test = self.collate((self.x_test))\n",
    "\n",
    "        X_train, y_train = [self.sent2features(s) for s in self.x_train], [self.sent2labels(s) for s in self.x_train]\n",
    "        X_test, y_test = [self.sent2features(s) for s in self.x_test], [self.sent2labels(s) for s in self.x_test]\n",
    "\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='l2sgd',  # l2sgd: Stochastic Gradient Descent with L2 regularization term\n",
    "            max_iterations=1000,  # maximum number of iterations\n",
    "        )\n",
    "        crf_model = crf.fit(X_train, y_train)\n",
    "        return  crf_model\n",
    "\n",
    "    #predict tags of given input sentence\n",
    "    def predict_tags(self, model, sentence):\n",
    "        text = []\n",
    "\n",
    "        #create list of workds from input sentence\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        text.append(tagged_words)\n",
    "\n",
    "        empty_list = word_tokenize(\"\")\n",
    "        empty_tagged_words = nltk.pos_tag(empty_list)\n",
    "        text.append(empty_tagged_words)\n",
    "\n",
    "        X_train = [self.sent2features(s) for s in text]\n",
    "        y = model.predict(X_train)\n",
    "\n",
    "        #remove any punctuation mark\n",
    "        if sentence[0][-1][0] in ['?', '!', '¨.']:\n",
    "            y[0][-1] = '0'\n",
    "\n",
    "        #Create output dataframe with two new columns and the predicted tag\n",
    "        tagged_words = np.asarray(tagged_words)\n",
    "        df_res = pd.DataFrame(tagged_words, columns=['WORD', 'POS'])\n",
    "        df_res['TAG'] = y[0]\n",
    "        return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class to identify the input question. Question content and type.\n",
    "class QuestionAnalyser:\n",
    "    def __init__(self):\n",
    "        print(\"Initialized Question Analyser\")\n",
    "\n",
    "    #Funciton to identify if question is about the director ot the actor.\n",
    "    #Counts nouns and verbs that are related to either. Role with highest score is designated as question content.\n",
    "    def identity_counter(self, lemma_verbs, lemma_nouns):\n",
    "\n",
    "        lemma_words = lemma_verbs + lemma_nouns\n",
    "\n",
    "        candidates = {\"director\" : ['director', \"filmmaker\",'direct', 'produce', 'filmed', 'lead'],\n",
    "                      \"actor\" : ['actor', 'role', 'star', 'act', 'play', 'star'],\n",
    "                      \"screenwriter\":  ['screenwriter', 'scriptwriter', 'author', 'screenwrite', 'write', 'script'],\n",
    "                      \"MPA film rating\" : ['mpaa', 'rating', 'ranking', 'grade', 'rate', 'rank', 'grade'],\n",
    "                      \"image\" : ['image', 'picture', 'face', 'look', \"show\"],\n",
    "                      \"genre\" : ['genre', 'type', 'style', 'category'],\n",
    "                      \"box office\" : [\"box\", \"office\", \"ticket\", \"booth\"],\n",
    "                      \"publication date\": [\"publication\", \"publish\", \"release\"],\n",
    "                      \"cast member\": [\"cast\", \"casting\"],\n",
    "                      \"producer\": [\"producer\", \"produce\", \"maker\", \"make\", \"production\"],\n",
    "                      \"executive producer\": [\"producer\", \"produce\", \"executive\", \"maker\", \"make\"],\n",
    "                      \"based on\": [\"base\", \"based\"],\n",
    "                      \"country of origin\" : [\"origin\"],\n",
    "                      \"filming location\": [\"location\", \"place\", \"locate\", \"film\", \"shoot\"],\n",
    "                      \"narrative location\": [\"location\", \"place\", \"locate\", \"narrate\", \"narrative\"],\n",
    "                      \"director of photography\": [\"photography\"],\n",
    "                      \"Box Office Mojo film ID\": [\"box\", \"office\", \"ticket\", \"booth\", \"Mojo\", \"ID\", \"id\"],\n",
    "                      \"set in period\": [\"period\"],\n",
    "                      \"duration\": [\"duration\"],\n",
    "                      \"film editor\": [\"editor\", \"edit\", \"compiler\", \"compile\"],\n",
    "                      \"FilmAffinity ID\": [\"FilmAffinity\", \"filmaffinity\"],\n",
    "                      \"Filmportal\": [\"Filmportal\", \"filmportal\"],\n",
    "                      \"main subject\": [\"subject\", \"topic\"],\n",
    "                      \"composer\": [\"composer\", \"compose\"],\n",
    "                      \"distributed by\": [\"distributer\", \"distribute\", \"trader\", \"trade\", \"seller\"],\n",
    "                      \"costume designer\": [\"costume\", \"outfit\", \"wardrobe\", \"dress\"],\n",
    "                      \"production designer\": [\"producer\", \"produce\", \"executive\", \"maker\", \"make\", \"designer\", \"design\"],\n",
    "                      \"Kinopoisk film ID\": [\"Kinopoisk\", \"kinopoisk\"],\n",
    "                      \"average shot length\": [\"average\", \"shot\", \"length\"],\n",
    "                      \"exploitation visa number\": [\"visa\", \"explotation\"],\n",
    "                      \"original film format\": [\"original\", \"film\", \"format\"],\n",
    "                      \"film script\": [\"script\", \"scripte\"],\n",
    "                      \"title\": [\"title\"],\n",
    "                      \"original language of film or TV show\": [\"original\", \"language\", \"initial\"],\n",
    "                      \"award received\": [\"award\"],\n",
    "                      \"media franchise\": [\"franchise\"],\n",
    "                      \"part of the series\": [\"series\"],\n",
    "                      \"budget\": [\"budget\", \"cost\"]\n",
    "                      }\n",
    "\n",
    "\n",
    "        counter = {\"director\": 0, \"actor\": 0, \"screenwriter\": 0, \"MPA film rating\": 0, \"image\": 0, \"genre\": 0,\n",
    "                   \"box office\": 0, \"publication date\": 0, \"cast member\": 0, \"producer\": 0, \"executive producer\":0,\n",
    "                   \"based on\": 0, \"country of origin\": 0, \"filming location\": 0, \"narrative location\": 0, \"director of photography\": 0,\n",
    "                   \"Box Office Mojo film ID\":0, \"set in period\": 0, \"duration\": 0, \"film editor\": 0, \"FilmAffinity ID\": 0,\n",
    "                   \"Filmportal ID\": 0, \"main subject\": 0, \"composer\": 0, \"distributed by\": 0,  \"costume designer\": 0,\n",
    "                   \"production designer\": 0, \"Kinopoisk film ID\": 0, \"exploitation visa number\": 0, \"average shot length\": 0,\n",
    "                   \"original film format\": 0, \"film script\": 0, \"title\": 0, \"award\": 0, \"media franchise\": 0, \"part of the series\": 0,\n",
    "                   \"budget\": 0\n",
    "                   }\n",
    "\n",
    "        #check occurences of words in input sentence\n",
    "        for key in candidates.keys():\n",
    "            for w in candidates[key]:\n",
    "                if w in lemma_words:\n",
    "                    counter[key] += 1\n",
    "\n",
    "        max_value = max(counter, key=counter.get)\n",
    "\n",
    "        #double check for box office and avg shot length since it should appear in word combinations\n",
    "        if max_value == \"Box Office Mojo film ID\":\n",
    "            lemma_lower = [w.lower() for w in lemma_words]\n",
    "            if \"id\" not in lemma_lower:\n",
    "                max_value = \"box office\"\n",
    "\n",
    "        if max_value == \"average shot length\":\n",
    "            counter[\"average shot length\"] -= 1\n",
    "            max_value = max(counter, key=counter.get)\n",
    "        if max_value == \"original film format\":\n",
    "            counter[\"original film format\"] -= 1\n",
    "            max_value = max(counter, key=counter.get)\n",
    "\n",
    "\n",
    "        #decide between location properties:\n",
    "        if max_value in [\"country of origin\", \"filming location\", \"narrative location\"]:\n",
    "            if \"origin\" in lemma_verbs:\n",
    "                max_value = \"country of origin\"\n",
    "            elif \"film\" or \"shoot\" in lemma_verbs:\n",
    "                max_value = \"filming location\"\n",
    "            else:\n",
    "                max_value = \"narrative location\"\n",
    "\n",
    "        #decide between the production rolees:\n",
    "        if max_value in [\"production designer\", \"executive producer\"]:\n",
    "            if \"designer\" in lemma_words:\n",
    "                max_value = \"production designer\"\n",
    "            elif  \"design\" in lemma_words:\n",
    "                max_value = \"production designer\"\n",
    "            elif \"executive\" in lemma_words:\n",
    "                max_value = \"executive producer\"\n",
    "            else: max_value = \"producer\"\n",
    "\n",
    "\n",
    "        return max_value\n",
    "\n",
    "\n",
    "\n",
    "    def identify_predicate(self, question):\n",
    "\n",
    "        # Get untaggde words\n",
    "        untagged_words = question[question[\"TAG\"]=='0']\n",
    "        verbs = untagged_words[untagged_words['POS'].isin(['VP', 'VBD', 'VBG', 'VBP', 'VBZ'])]['WORD']\n",
    "        nouns = untagged_words[untagged_words['POS'].isin(['NN', 'NNP', 'NNPS', \"JJ\"])]['WORD']\n",
    "\n",
    "        #Lemmatize means to get the root of the word\n",
    "        lemma = WordNetLemmatizer()\n",
    "        lemma_verbs = [lemma.lemmatize(v) for v in verbs]\n",
    "        lemma_nouns = [lemma.lemmatize(v) for v in nouns]\n",
    "        print(lemma_nouns)\n",
    "        print(lemma_verbs)\n",
    "\n",
    "        ########################################################\n",
    "        pred = self.identity_counter(lemma_nouns = lemma_nouns, lemma_verbs=lemma_verbs)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TypoCorrector:\n",
    "\n",
    "    @staticmethod\n",
    "    def change_minus_sign(input):\n",
    "        # replace minus with dash\n",
    "        if \"-\" in input:\n",
    "            input = input.replace(\"-\", \"–\")\n",
    "\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def change_dash_sign(input):\n",
    "        # replace minus with dash\n",
    "        if \"-\" in input:\n",
    "            input = input.replace(\"–\",\"-\")\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class  ModelTrainer:\n",
    "\n",
    "    def __init__(self, ner_model, training_data, test_data):\n",
    "        self.ner_model = ner_model\n",
    "        self.x_train = pd.read_csv(training_data, encoding=\"ISO-8859-1\")\n",
    "        self.x_test = pd.read_csv(test_data, encoding=\"ISO-8859-1\")\n",
    "        print(\"Loaded data successfully\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        self.ner_model.train(self.x_train,self.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Crowdsourcing:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv('D:/deanh/Uni/Master/HS22/Advanced AI/Project/crowdsourcing/crowd_data.tsv', sep='\\t')\n",
    "\n",
    "    def clean_crowdsourcingfile(self):\n",
    "        #remove % sign from approvalrate to handle as integer\n",
    "        self.df[\"LifetimeApprovalRate\"] = [int(word[:-1]) for word in self.df['LifetimeApprovalRate']]\n",
    "\n",
    "        # We define upper and lower bounds for response time and approval rate by using mean and standard deviation\n",
    "        # In believe that workers outside of these bounds are malicious. We add +5% to all bounds to include valid edge cases\n",
    "        # We also drop rows that indicate as CORRECT but say the know an error and where to handle it\n",
    "\n",
    "        x = \"WorkTimeInSeconds\"\n",
    "        y = \"LifetimeApprovalRate\"\n",
    "\n",
    "        mean_approvalRate = self.df[y].mean()\n",
    "        std_approvalRate = self.df[y].std()\n",
    "\n",
    "        mean_responseTime = self.df[x].mean()\n",
    "        std_responseTime = self.df[x].std()\n",
    "\n",
    "        upperbound_responseTime = mean_responseTime + std_responseTime + 5\n",
    "        lowerbound_responseTime = mean_responseTime - std_responseTime - 5\n",
    "\n",
    "        upperbound_approvalRate = mean_approvalRate + std_approvalRate + 5\n",
    "        lowerbound_approvalRate = mean_approvalRate - std_approvalRate -5\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds > upperbound_responseTime)].index)\n",
    "        self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds < lowerbound_responseTime)].index)\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate > upperbound_approvalRate)].index)\n",
    "        self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate < lowerbound_approvalRate)].index)\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df['AnswerLabel'] == \"CORRECT\") & (self.df[\"FixPosition\"] == \"yes\")].index)\n",
    "\n",
    "        #There is a type in Input2ID column\n",
    "        self.df[\"Input2ID\"] = self.df[\"Input2ID\"].replace(\"wdt:.P344\", \"wdt:P344\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def crowdsourcing_answer(self, sub, pred):\n",
    "        rows = self.df.loc[self.df[\"Input1ID\"] == \"wd:\"+sub]\n",
    "        rows = rows.loc[rows[\"Input2ID\"] == \"wdt:\"+pred]\n",
    "\n",
    "        agree = rows.loc[rows[\"AnswerLabel\"] == \"CORRECT\"]\n",
    "        disagree = rows.loc[rows[\"AnswerLabel\"] == \"INCORRECT\"]\n",
    "        value = rows.iloc[0]['Input3ID']\n",
    "\n",
    "        if agree.shape[0] > disagree.shape[0]:\n",
    "            return(\"The crowd agrees that the answer is \" + str(value))\n",
    "        if agree.shape[0] < disagree.shape[0]:\n",
    "            return(\"The crowd disagrees that the answer is \" + str(value))\n",
    "        else:\n",
    "            return(\"The crowd is uncertain that the answer is \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KGLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_KG(self):\n",
    "        print(\"Loading knowledge graph from source\")\n",
    "        movie_graph = Graph()\n",
    "        movie_graph.parse(source='datasets/14_graph.nt', format='turtle')\n",
    "        print(\"succesfully loaded knowledge graph\")\n",
    "        return movie_graph\n",
    "\n",
    "    def load_embeddings(self, graph):\n",
    "        #add path parameter to retrive dictionaries?\n",
    "\n",
    "        DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "        RDFS = rdflib.namespace.RDFS\n",
    "        SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "\n",
    "        # load the embeddings\n",
    "        entity_emb = np.load(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'entity_embeds.npy'))\n",
    "        relation_emb = np.load(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'relation_embeds.npy'))\n",
    "\n",
    "        # load the dictionaries\n",
    "        with open(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'entity_ids.del'), 'r') as ifile:\n",
    "            ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "            id2ent = {v: k for k, v in ent2id.items()}\n",
    "        with open(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'relation_ids.del'), 'r') as ifile:\n",
    "            rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "            id2rel = {v: k for k, v in rel2id.items()}\n",
    "\n",
    "        ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
    "        lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
    "\n",
    "        embedding = (entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Returns a SQL statement as a string\n",
    "class QueryProducer:\n",
    "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "    WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def director_query(self, title):\n",
    "        query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:P57 ?director .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "        return query\n",
    "\n",
    "    # returns a dict, with all film titles and their entity Id\n",
    "    def all_films_id_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "        PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX p: <http://www.wikidata.org/prop/>\n",
    "        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?item ?label\n",
    "            WHERE{\n",
    "            ?item wdt:P31 <http://www.wikidata.org/entity/Q11424> .\n",
    "            ?item rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = \"en\").\n",
    "        }\n",
    "                    '''\n",
    "        response = moviegraph.query(query)\n",
    "        id_to_film = {e[len(WD):]: str(i) for e,i in response}\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "\n",
    "        return  film_to_id\n",
    "\n",
    "        # returns a dict, with all anime titles and their entity Id\n",
    "    def all_animes_id_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "        PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX p: <http://www.wikidata.org/prop/>\n",
    "        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?item ?label\n",
    "            WHERE{\n",
    "            ?item wdt:P31 <http://www.wikidata.org/entity/Q20650540> .\n",
    "            ?item rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = \"en\").\n",
    "        }\n",
    "                    '''\n",
    "        response = moviegraph.query(query)\n",
    "        id_to_film = {e[len(WD):]: str(i) for e,i in response}\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "\n",
    "        return  film_to_id\n",
    "\n",
    "    def all_actors_id_and_IMDb_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        query ='''\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "            PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX p: <http://www.wikidata.org/prop/>\n",
    "            PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "            PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "            SELECT ?item ?label ?IMDb_ID\n",
    "                WHERE{\n",
    "                ?item wdt:P31 wd:Q5 .\n",
    "                ?item wdt:P106 wd:Q33999 .\n",
    "                ?item rdfs:label ?label .\n",
    "                ?item wdt:P345 ?IMDb_ID  .\n",
    "            }\n",
    "\n",
    "\n",
    "            '''\n",
    "\n",
    "        response = moviegraph.query(query)\n",
    "        ID_to_actor = {e[len(WD):]: str(i) for e,i, IDMb in response}\n",
    "        actor_to_Id = {value:key for key, value in ID_to_actor.items()}\n",
    "\n",
    "        ID_to_actor = {e[len(WD):]: str(IDMb) for e,i, IDMb in response}\n",
    "\n",
    "        return [actor_to_Id, ID_to_actor]\n",
    "\n",
    "    def predicateID_query(self, proposition):\n",
    "        query =\"\"\"\n",
    "\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        SELECT ?obj WHERE {\n",
    "            ?obj rdfs:label \"\"\" + '\"' + proposition + '\"' + \"\"\"@en .\n",
    "\n",
    "        }\n",
    "        \"\"\"\n",
    "        entity =moviegraph.query(query)\n",
    "        df = pd.DataFrame(entity.bindings)\n",
    "        url = str((df.iloc[0][0]))\n",
    "        id = url[len(QueryProducer.WDT):]\n",
    "\n",
    "        return id\n",
    "\n",
    "    def any_property_value_query(self, title, id, propertyname):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        \n",
    "        if \" \" in propertyname:\n",
    "            propertyname = propertyname.replace(\" \", \"_\")\n",
    "\n",
    "        query ='''\n",
    "                SELECT ?name\n",
    "                WHERE {\n",
    "                ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                      rdfs:label \"''' + title + '''\"@en ;\n",
    "                      ns1:'''+id+''' ?'''+propertyname+''' .\n",
    "                ?'''+propertyname+''' rdfs:label ?name\n",
    "                }\n",
    "                '''\n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge graph from source\n",
      "succesfully loaded knowledge graph\n",
      "loaded KG and embeddings\n"
     ]
    }
   ],
   "source": [
    "KGloader = KGLoader()\n",
    "moviegraph = KGloader.load_KG()\n",
    "embedding = KGloader.load_embeddings(moviegraph)\n",
    "print(\"loaded KG and embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded film and actor dicts\n"
     ]
    }
   ],
   "source": [
    "Q = QueryProducer()\n",
    "anime_dict = Q.all_animes_id_query()\n",
    "films_dict = Q.all_films_id_query()\n",
    "actors_ID_dict = Q.all_actors_id_and_IMDb_query()[0]\n",
    "actor_IMDb_dict = Q.all_actors_id_and_IMDb_query()[1]\n",
    "print(\"loaded film and actor dicts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded image file\n"
     ]
    }
   ],
   "source": [
    "image_frame = pd.DataFrame.from_dict(json.load(open(\"images.json\")))\n",
    "print(\"loaded image file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded crowdsourcing file\n"
     ]
    }
   ],
   "source": [
    "C = Crowdsourcing()\n",
    "clean_crowdsourcingfile = C.clean_crowdsourcingfile()\n",
    "print(\"loaded crowdsourcing file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupHandler:\n",
    "    def __init__(self, film_ids, actor_ids, actor_IMDb, query_producer, moviegraph, embedding, images, crowdsourcing):\n",
    "        self.film_ids = film_ids\n",
    "        self.actor_IDs = actor_ids\n",
    "        self.actor_IMDb = actor_IMDb\n",
    "        self.query_producer = query_producer\n",
    "        self.movie_graph = moviegraph\n",
    "        self.embedding = embedding\n",
    "        self.images = images\n",
    "        self.crowdsourcing = crowdsourcing\n",
    "    \n",
    "    def film_id_finder(self, film_title):\n",
    "        # Film entity. Using lowercase title for lookup\n",
    "        film_title = film_title.lower()\n",
    "        try:\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            return film_id\n",
    "        except:\n",
    "            print(\"could not find film, trying to find it correcting typos...\")\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.change_minus_sign(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            return film_id\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.change_dash_sign(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            return film_id\n",
    "        except:\n",
    "            print(film_title)\n",
    "            raise KeyError(\"Film title contains typos or does not exist!\")\n",
    "\n",
    "        return film_id\n",
    "\n",
    "    def graph_lookup(self, film_title, pred):\n",
    "        \n",
    "        pred_id = self.query_producer.predicateID_query(pred)\n",
    "        entity = \"\"\n",
    "        query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "        entity = self.movie_graph.query(query)\n",
    "        df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "        if df_entity.empty:\n",
    "            film_title = TypoCorrector.change_minus_sign(film_title)\n",
    "            query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "            entity = self.movie_graph.query(query)\n",
    "            df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "            if df_entity.empty:\n",
    "                film_title = TypoCorrector.change_dash_sign(film_title)\n",
    "                query = self.query_producer.any_property_value_query(title=film_title, id=pred_id, propertyname=pred)\n",
    "                entity = self.movie_graph.query(query)\n",
    "                df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "        # takes all answers but responds with the first one\n",
    "        property_value = []\n",
    "        for _ in entity:\n",
    "            property_value.append(str(_.name))\n",
    "        return property_value\n",
    "\n",
    "    def embedding_lookup(self, film_title, predicate):\n",
    "        entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent = self.embedding\n",
    "\n",
    "        # define some prefixes\n",
    "        WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "        RDFS = rdflib.namespace.RDFS\n",
    "        SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "\n",
    "        film_id = self.film_id_finder(film_title)\n",
    "        head = entity_emb[ent2id[WD[film_id]]]\n",
    "\n",
    "        if predicate =='genre':\n",
    "            # \"genre\" relation\n",
    "            pred = relation_emb[rel2id[WDT.P136]]\n",
    "        if predicate == 'rating':\n",
    "            # \"MPAA film rating\" relation\n",
    "            pred = relation_emb[rel2id[WDT.P1657]]\n",
    "        if predicate == 'screenwriter':\n",
    "            # \"screenwriter \" relation\n",
    "            pred = relation_emb[rel2id[WDT.P58]]\n",
    "\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "        # compute ranks of entities\n",
    "        ranks = dist.argsort().argsort()\n",
    "\n",
    "        top_ten_results = pd.DataFrame([\n",
    "                (id2ent[idx][len(WD):], ent2lbl[id2ent[idx]], dist[idx], rank+1)\n",
    "                for rank, idx in enumerate(most_likely[:10])],\n",
    "                columns=('Entity', 'Label', 'Score', 'Rank'))\n",
    "        best_result = top_ten_results.iloc[0:2][\"Label\"]\n",
    "        best_result = best_result\n",
    "\n",
    "        return best_result\n",
    "\n",
    "    def image_lookup(self, name, asFilmIdentifiedName):\n",
    "        try:\n",
    "            actor_Id = self.actor_IDs[name]\n",
    "        except:\n",
    "            actor_Id = self.actor_IDs[asFilmIdentifiedName]\n",
    "\n",
    "        IMDb_Id = self.actor_IMDb[actor_Id]\n",
    "        valid_options = self.images.loc[self.images[\"cast\"].str.contains(IMDb_Id, regex=False)]\n",
    "    \n",
    "        #only images where actor appears alone and its a poster. Take a random sample\n",
    "        valid_options = valid_options.loc[valid_options[\"type\"]==\"poster\"]\n",
    "        valid_options = valid_options.query(\"cast.str.len() == 1\", engine=\"python\")\n",
    "        valid_options = valid_options[\"img\"]\n",
    "        valid_options = valid_options.sample()\n",
    "\n",
    "        url = str(valid_options.iloc[0])\n",
    "        return url\n",
    "\n",
    "    def crowdsourcing_lookup(self, filmtitle, pred_id):\n",
    "        film_id = self.film_id_finder(filmtitle)\n",
    "        answer = self.crowdsourcing.crowdsourcing_answer(film_id, pred_id)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Takes an argument \"training\" to determine if it should create and save a model or load an existing one\n",
    "class RunProcess:\n",
    "    def __init__(self, training, knowledgegraph):\n",
    "        print(\"Starting process\")\n",
    "        self.ner_model = NerTrainer()\n",
    "        if training:\n",
    "            self.crf_model = self.ner_model.train()\n",
    "            print(\"Created and trained model succesfully\")\n",
    "            with open('crf_model.pkl', 'wb') as f:\n",
    "                pickle.dump(self.crf_model, f)\n",
    "            print(\"Saved crf-model\")\n",
    "        else:\n",
    "            #self.ner_model = NerDLModel.pretrained('ner_mit_movie_simple_distilbert_base_cased', 'en')\\\n",
    "            #.setInputCols(['document', 'token', 'embeddings']).setOutputCol('ner')\n",
    "            with open('crf_model.pkl', 'rb') as f:\n",
    "                self.crf_model = pickle.load(f)\n",
    "            print(\"loaded ner-mnodel\")\n",
    "\n",
    "        self.question_analyser = QuestionAnalyser()\n",
    "        self.query_producer = QueryProducer()\n",
    "        print(\"Queryproduce initialized\")\n",
    "\n",
    "        self.movie_graph = knowledgegraph\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.anime_ids = anime_dict\n",
    "        self.normal_film_ids =films_dict\n",
    "        self.film_ids = {**self.anime_ids, **self.normal_film_ids} #combine both to one dict\n",
    "        print(\"created film dict\")\n",
    "\n",
    "        self.crowdourcing = Crowdsourcing()\n",
    "\n",
    "        self.actor_IDs = actors_ID_dict\n",
    "        self.actor_IMDb = actor_IMDb_dict\n",
    "\n",
    "        print(\"waiting for image file\")\n",
    "        self.images = image_frame\n",
    "\n",
    "        self.lookup_handler = LookupHandler(self.film_ids, self.actor_IDs, self.actor_IMDb, \n",
    "            self.query_producer, self.movie_graph, self.embedding, self.images, self.crowdourcing)\n",
    "\n",
    "        print(\"----------Run Initialized!----------\")\n",
    "\n",
    "    #\n",
    "    def ask_question(self):\n",
    "        question = input(\"Please ask your question\")\n",
    "        return question\n",
    "\n",
    "    def identify_subject_and_predicate(self, question):\n",
    "        tagged_entities = self.ner_model.predict_tags(self.crf_model, question)\n",
    "        print(\"Sentence with tags: \")\n",
    "        print(tagged_entities)\n",
    "        print(\"#################################################################\")\n",
    "        identified_predicate = self.question_analyser.identify_predicate(tagged_entities)\n",
    "\n",
    "        #TODO Here we can also check for actors. ALSO we can check if I-title is a questions mark, comma etc.\n",
    "\n",
    "        film_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-TITLE','I-TITLE'])]\n",
    "        film_title = ''\n",
    "        for w in film_wordlist:\n",
    "            film_title = film_title + w + ' '\n",
    "\n",
    "        actor_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-ACTOR','I-ACTOR'])]\n",
    "        actor = ''\n",
    "        for w in actor_wordlist:\n",
    "            actor = actor + w + ' '\n",
    "        # need to remove empry string at end of concatenation\n",
    "        film_title = film_title[:-1]\n",
    "        actor = actor[:-1]\n",
    "\n",
    "        print(\"Asking for: \" + identified_predicate + \" in: \" + film_title + \" or actor : \" + actor)\n",
    "        return film_title, actor, identified_predicate\n",
    "\n",
    "\n",
    "        film_id = self.film_id_finder(filmtitle)\n",
    "        answer = self.crowdourcing.crowdsourcing_answer(film_id, pred_id)\n",
    "        return answer\n",
    "\n",
    "    def answer_movie_question(self):\n",
    "        question = self.ask_question()\n",
    "\n",
    "        film_title, actor, pred = self.identify_subject_and_predicate(question)\n",
    "\n",
    "        pred_id = self.query_producer.predicateID_query(pred)\n",
    "\n",
    "        print(\"subject: \"+film_title + \" | actor: \" +actor+ \" | predicate: \"+pred+\" | predicate ID: \"+pred_id ) \n",
    "        print(\"##################################################################\")\n",
    "\n",
    "        # To perform the query, we must remove punctuations.\n",
    "\n",
    "        if len(film_title) >= 1:\n",
    "            if film_title[-1] in ['?', '!', '.']:\n",
    "                film_title = film_title[:-2]\n",
    "        # intermediate punctions have a space in front of them that needs to be removed\n",
    "        if \" :\" in film_title:\n",
    "            film_title = film_title.replace(\" :\", \":\")\n",
    "\n",
    "        if len(actor) >= 1:\n",
    "            if actor[-1] in ['?', '!', '.']:\n",
    "                actor = actor[:-2]\n",
    "\n",
    "        #########################################################################\n",
    "\n",
    "        if pred == 'image':\n",
    "           image_url = self.lookup_handler.image_lookup(name = actor, asFilmIdentifiedName=film_title)\n",
    "           print(\"Image of \" + film_title + actor + \" at this url: \"+ image_url)\n",
    "\n",
    "        else: \n",
    "            try:\n",
    "                answer_crowd = self.lookup_handler.crowdsourcing_lookup(film_title, pred_id)\n",
    "                print(\"answer from the crowd \" + str(answer_crowd))\n",
    "            except:\n",
    "                print(\"Not found in crowdsourcing\")\n",
    "                pass\n",
    "\n",
    "            try: \n",
    "                answer_graph = self.lookup_handler.graph_lookup(film_title, pred)\n",
    "                print(\"answer of graph lookup \" + answer_graph[0])\n",
    "            except:\n",
    "                print(\"Not found in graph\")\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                answer_embedding = self.lookup_handler.embedding_lookup(film_title, pred)\n",
    "                print(\"embedding answer: \")\n",
    "                print(answer_embedding)\n",
    "            except:\n",
    "                print(\"Could not find: \" + film_title)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process\n",
      "Loaded data & initialized NER-model\n",
      "loaded ner-mnodel\n",
      "Initialized Question Analyser\n",
      "Queryproduce initialized\n",
      "created film dict\n",
      "waiting for image file\n",
      "----------Run Initialized!----------\n"
     ]
    }
   ],
   "source": [
    "Process3 = RunProcess(training=False, knowledgegraph=moviegraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with tags: \n",
      "       WORD  POS      TAG\n",
      "0       Who   WP        0\n",
      "1  directed  VBD        0\n",
      "2       The   DT  B-TITLE\n",
      "3      Lion  NNP  I-TITLE\n",
      "4      King  NNP  I-TITLE\n",
      "5         ?    .  I-TITLE\n",
      "#################################################################\n",
      "[]\n",
      "['directed']\n",
      "Asking for: director in: The Lion King ? or actor : \n",
      "subject: The Lion King ? | actor:  | predicate: director | predicate ID: P57\n",
      "##################################################################\n",
      "Not found in crowdsourcing\n",
      "Not found in graph\n",
      "Could not find: The Lion King\n"
     ]
    }
   ],
   "source": [
    "Process3.answer_movie_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWho is the director of Good Will Hunting?\\nWho is the director of Titanic?\\n\\nWho is the screenwriter of The Masked Gang: Cyprus\\nWho is the screenwriter of Titanic?\\nWho is the screenwriter of Weathering with You\\n\\nWho is the director of Star Wars: Episode VI - Return of the Jedi?\\nWho is the executive producer of X-Men: First Class?\\n\\nWhat is the MPAA film rating of Weathering with You?\\nWhat is the MPAA film rating of Titanic?\\nWhat is the box office of The Princess and the Frog?\\nCan you tell me the publication date of Tom Meets Zizou?\\n\\nThe Lion King\\nThe Notebook\\nTitanic\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Who is the director of Good Will Hunting?\n",
    "Who is the director of Titanic?\n",
    "Who produced The Lion King?\n",
    "\n",
    "\n",
    "Who is the screenwriter of The Masked Gang: Cyprus\n",
    "Who is the screenwriter of Titanic?\n",
    "Who is the screenwriter of Weathering with You\n",
    "\n",
    "Who is the director of Star Wars: Episode VI - Return of the Jedi?\n",
    "Who is the executive producer of X-Men: First Class?\n",
    "What is the genre of The Lion King?\n",
    "\n",
    "What is the MPAA film rating of Weathering with You?\n",
    "What is the MPAA film rating of Titanic?\n",
    "What is the box office of The Princess and the Frog?\n",
    "Can you tell me the publication date of Tom Meets Zizou?\n",
    "\n",
    "Show me a picture of Julia Roberts.\n",
    "\n",
    "The Lion King\n",
    "The Notebook\n",
    "Titanic\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def predicateID_query(proposition):\n",
    "    if \" \" in proposition:\n",
    "        proposition.replace(\" \", \"_\")\n",
    "\n",
    "        query =\"\"\"\n",
    "\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        SELECT ?obj WHERE {\n",
    "            ?obj rdfs:label \"\"\" + '\"' + proposition + '\"' + \"\"\"@en .\n",
    "\n",
    "        }\n",
    "        \"\"\"\n",
    "        entity =moviegraph.query(query)\n",
    "        print(url)\n",
    "        df = pd.DataFrame(entity.bindings)\n",
    "        url = str((df.iloc[0][0]))\n",
    "        id = url[len(QueryProducer.WDT):]\n",
    "\n",
    "        return id\n",
    "\n",
    "print(predicateID_query(\"genre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"(rdflib.term.Literal('Johnny English film series', lang='en'),)\"}\n"
     ]
    }
   ],
   "source": [
    "def any_property(title, id, propertyname):\n",
    "    if \" \" in propertyname:\n",
    "        propertyname = propertyname.replace(\" \", \"_\")\n",
    "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "    WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "    query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:'''+id+''' ?'''+propertyname+''' .\n",
    "            ?'''+propertyname+''' rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "    response = moviegraph.query(query)\n",
    "    response = { str(i) for i in response}\n",
    "\n",
    "    return  response\n",
    "re = any_property(\"Johnny English\", \"P179\", \"part of the series\")\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = pd.DataFrame(dict, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shit = \"x–men: first class\"\n",
    "\n",
    "ok = \"x-men: first class\"\n",
    "\n",
    "shiteee = \"-\"\n",
    "okeier = \"-\"\n",
    "print(shiteee == okeier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(keys[ok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "title = \"Star Wars: Episode VI – Return of the Jedi\"\n",
    "\n",
    "query ='''\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "            PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX p: <http://www.wikidata.org/prop/>\n",
    "            PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "            PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film wdt:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  wdt:P57 ?director .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "\n",
    "\n",
    "            '''\n",
    "\n",
    "response = moviegraph.query(query)\n",
    "response = { str(i) for i in response}\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ID_to_actor = {e[len(WD):]: str(i) for e,i, IDMb in response}\n",
    "actor_to_Id = {value:key for key, value in ID_to_actor.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "result = pd.DataFrame([\n",
    "    (id[len(WD):], label, ipbm_id)\n",
    "    for id, label, ipbm_id in response],\n",
    "    columns=('ID', \"label\", 'Imdb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = \"Lord of the Rings : the two towers\"\n",
    "\n",
    "s1 = s.replace(\" :\", \":\")\n",
    "\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "949167c31c494cbe6f2153fb142c4e45576b141632160fdf2c3b2b305db6da2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
