{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from rdflib import Graph, URIRef\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import json\n",
    "import time\n",
    "import atexit\n",
    "import getpass\n",
    "import requests  # install the package via \"pip install requests\"\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "import rdflib\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#from sparknlp.annotator import *\n",
    "#from sparknlp.base import *\n",
    "#import sparknlp\n",
    "#import spark\n",
    "import urllib.request\n",
    "\n",
    "#from sparknlp.annotator import *\n",
    "#from sparknlp.base import *\n",
    "#import sparknlp\n",
    "#spark = sparknlp.start()\n",
    "\n",
    "#from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "###\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Answer factual questions\n",
    "Model training need to be applied only once. The function is at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NerTrainer:\n",
    "    #Most of the funcitons are from the Graphical Models Tutorial from the class\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_train = pd.read_csv('engtrain_clean.csv', encoding=\"ISO-8859-1\")\n",
    "        self.x_test = pd.read_csv('engtest_clean.csv', encoding=\"ISO-8859-1\")\n",
    "        print(\"Loaded data & initialized NER-model\")\n",
    "\n",
    "    def collate(self, dataframe):\n",
    "        agg_func = lambda s: [(w, pos, t) for w, pos, t in\n",
    "                              zip(s['WORD'].values.tolist(), s['POS'].values.tolist(), s['ï»¿TAG'].values.tolist())]\n",
    "        grouped = dataframe.groupby('SENTENCE').apply(agg_func)\n",
    "        return list(grouped)\n",
    "\n",
    "    # counts sentences in given input data\n",
    "    def count_sentences(self, data):\n",
    "        counter = 0\n",
    "        for index, row in data.iterrows():\n",
    "            data.at[index, 'SENTENCE'] = counter\n",
    "            if pd.isna(data.iloc[index, 0]):\n",
    "                counter += 1\n",
    "        data[\"POS\"] = \"0\"\n",
    "        data = data.dropna()\n",
    "        data = data.reset_index(drop=True)\n",
    "        return data\n",
    "\n",
    "    def word2features(self, sent, i):\n",
    "        word = sent[i][0]\n",
    "        postag = sent[i][1]\n",
    "\n",
    "        features = {\n",
    "            'word.lower()': word.lower(),  # the word in lowercase\n",
    "            'word[-3:]': word[-3:],  # last three characters\n",
    "            'word[-2:]': word[-2:],  # last two characters\n",
    "            'word.isupper()': word.isupper(),  # true, if the word is in uppercase\n",
    "            'word.istitle()': word.istitle(),\n",
    "            # true, if the first character is in uppercase and remaining characters are in lowercase\n",
    "            'word.isdigit()': word.isdigit(),  # true, if all characters are digits\n",
    "            'postag': postag,  # POS tag\n",
    "            'postag[:2]': postag[:2],  # IOB prefix\n",
    "        }\n",
    "\n",
    "        if i > 0:\n",
    "            word1 = sent[i - 1][0]  # the previous word\n",
    "            postag1 = sent[i - 1][1]  # POS tag of the previous word\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2],\n",
    "            })  # add some features of the previous word\n",
    "        else:\n",
    "            features['BOS'] = True  # BOS: begining of the sentence\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i + 1][0]  # the next word\n",
    "            postag1 = sent[i + 1][1]  # POS tag of the next word\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2],\n",
    "            })  # add some features of the next word\n",
    "        else:\n",
    "            features['EOS'] = True  # EOS: end of the sentence\n",
    "        return features\n",
    "\n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for _, _, label in sent]\n",
    "\n",
    "    def tag_data(self, data_to_tag):\n",
    "        tags = []\n",
    "        x_train = data_to_tag.groupby(['SENTENCE'])\n",
    "        x_train = x_train.apply(np.mean)\n",
    "\n",
    "        # x_train.shape[0] will give us the number of sentences to loop through and tag them using nltk lib\n",
    "        # first we locate the sentence in our input data, then we hand the sentence as a list of words to the nltk function which tags the words.\n",
    "        for i in range(x_train.shape[0]):\n",
    "            sentence = data_to_tag.loc[data_to_tag[\"SENTENCE\"] == i]\n",
    "            sentence = sentence['WORD'].tolist()\n",
    "            tag = nltk.pos_tag(sentence)\n",
    "            tags.append(tag)\n",
    "\n",
    "        #now we add the found tags to the actual data in the data set\n",
    "        counter = 0\n",
    "        for sentence in tags:\n",
    "            for word in sentence:\n",
    "                data_to_tag.at[counter, 'POS'] = word[1]\n",
    "                counter += 1\n",
    "        print(\"data to tag in \"\"tag_data()\"\":\" )\n",
    "        print(data_to_tag)\n",
    "        print(\"#######################################################\")\n",
    "        return data_to_tag\n",
    "\n",
    "    #code copied from graphical_model tutorial and collected under one train function\n",
    "    def train(self):\n",
    "        self.x_train = self.count_sentences(self.x_train)\n",
    "        self.x_test = self.count_sentences(self.x_test)\n",
    "\n",
    "        self.x_train = self.tag_data(self.x_train)\n",
    "        self.x_test = self.tag_data((self.x_test))\n",
    "\n",
    "        self.x_train = self.collate(self.x_train)\n",
    "        self.x_test = self.collate((self.x_test))\n",
    "\n",
    "        X_train, y_train = [self.sent2features(s) for s in self.x_train], [self.sent2labels(s) for s in self.x_train]\n",
    "        X_test, y_test = [self.sent2features(s) for s in self.x_test], [self.sent2labels(s) for s in self.x_test]\n",
    "\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='l2sgd',  # l2sgd: Stochastic Gradient Descent with L2 regularization term\n",
    "            max_iterations=1000,  # maximum number of iterations\n",
    "        )\n",
    "        crf_model = crf.fit(X_train, y_train)\n",
    "        return  crf_model\n",
    "\n",
    "    #predict tags of given input sentence\n",
    "    def predict_tags(self, model, sentence):\n",
    "        text = []\n",
    "\n",
    "        #create list of workds from input sentence\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        text.append(tagged_words)\n",
    "\n",
    "        empty_list = word_tokenize(\"\")\n",
    "        empty_tagged_words = nltk.pos_tag(empty_list)\n",
    "        text.append(empty_tagged_words)\n",
    "\n",
    "        X_train = [self.sent2features(s) for s in text]\n",
    "        y = model.predict(X_train)\n",
    "\n",
    "        #remove any punctuation mark\n",
    "        if sentence[0][-1][0] in ['?', '!', '¨.']:\n",
    "            y[0][-1] = '0'\n",
    "\n",
    "        #Create output dataframe with two new columns and the predicted tag\n",
    "        tagged_words = np.asarray(tagged_words)\n",
    "        df_res = pd.DataFrame(tagged_words, columns=['WORD', 'POS'])\n",
    "        df_res['TAG'] = y[0]\n",
    "        return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class to identify the input question. Question content and type.\n",
    "class QuestionAnalyser:\n",
    "    def __init__(self):\n",
    "        print(\"Initialized Question Analyser\")\n",
    "\n",
    "    #Funciton to identify if question is about the director ot the actor.\n",
    "    #Counts nouns and verbs that are related to either. Role with highest score is designated as question content.\n",
    "    def identity_counter(self, lemma_verbs, lemma_nouns):\n",
    "\n",
    "        lemma_words = lemma_verbs + lemma_nouns\n",
    "\n",
    "        candidates = {\"director\" : ['director', \"filmmaker\",'direct', 'produce', 'filmed', 'lead'],\n",
    "                      \"actor\" : ['actor', 'role', 'star', 'act', 'play', 'star'],\n",
    "                      \"screenwriter\":  ['screenwriter', 'scriptwriter', 'author', 'screenwrite', 'write', 'script'],\n",
    "                      \"rating\" : ['mpaa', 'rating', 'ranking', 'grade', 'rate', 'rank', 'grade'],\n",
    "                      \"image\" : ['image', 'picture', 'face', 'look'],\n",
    "                      \"genre\" : ['genre', 'type', 'style', 'category'],\n",
    "                      \"box office\" : [\"box\", \"office\", \"ticket\", \"booth\"],\n",
    "                      \"publication date\": [\"publication\", \"publish\", \"release\"],\n",
    "                      \"cast member\": [\"cast\", \"casting\"],\n",
    "                      \"producer\": [\"producer\", \"produce\", \"maker\", \"make\", \"production\"],\n",
    "                      \"executive producer\": [\"producer\", \"produce\", \"executive\", \"maker\", \"make\"],\n",
    "                      \"based on\": [\"base\", \"based\"],\n",
    "                      \"country of origin\" : [\"origin\"],\n",
    "                      \"filming location\": [\"location\", \"place\", \"locate\", \"film\", \"shoot\"],\n",
    "                      \"narrative location\": [\"location\", \"place\", \"locate\", \"narrate\", \"narrative\"],\n",
    "                      \"director of photography\": [\"photography\"],\n",
    "                      \"Box Office Mojo film ID\": [\"box\", \"office\", \"ticket\", \"booth\", \"Mojo\", \"ID\", \"id\"],\n",
    "                      \"set in period\": [\"period\"],\n",
    "                      \"duration\": [\"duration\"],\n",
    "                      \"film editor\": [\"editor\", \"edit\", \"compiler\", \"compile\"],\n",
    "                      \"FilmAffinity ID\": [\"FilmAffinity\", \"filmaffinity\"],\n",
    "                      \"Filmportal\": [\"Filmportal\", \"filmportal\"],\n",
    "                      \"main subject\": [\"subject\", \"topic\"],\n",
    "                      \"composer\": [\"composer\", \"compose\"],\n",
    "                      \"distributed by\": [\"distributer\", \"distribute\", \"trader\", \"trade\", \"seller\"],\n",
    "                      \"costume designer\": [\"costume\", \"outfit\", \"wardrobe\", \"dress\"],\n",
    "                      \"production designer\": [\"producer\", \"produce\", \"executive\", \"maker\", \"make\", \"designer\", \"design\"],\n",
    "                      \"Kinopoisk film ID\": [\"Kinopoisk\", \"kinopoisk\"],\n",
    "                      \"average shot length\": [\"average\", \"shot\", \"length\"],\n",
    "                      \"exploitation visa number\": [\"visa\", \"explotation\"],\n",
    "                      \"original film format\": [\"original\", \"film\", \"format\"],\n",
    "                      \"film script\": [\"script\", \"scripte\"],\n",
    "                      \"title\": [\"title\"],\n",
    "                      \"original language of film or TV show\": [\"original\", \"language\", \"initial\"],\n",
    "                      \"award received\": [\"award\"],\n",
    "                      \"media franchise\": [\"franchise\"],\n",
    "                      \"part of the series\": [\"series\"],\n",
    "                      \"budget\": [\"budget\", \"cost\"]\n",
    "                      }\n",
    "\n",
    "\n",
    "        counter = {\"director\": 0, \"actor\": 0, \"screenwriter\": 0, \"rating\": 0, \"image\": 0, \"genre\": 0,\n",
    "                   \"box office\": 0, \"publication date\": 0, \"cast member\": 0, \"producer\": 0, \"executive producer\":0,\n",
    "                   \"based on\": 0, \"country of origin\": 0, \"filming location\": 0, \"narrative location\": 0, \"director of photography\": 0,\n",
    "                   \"Box Office Mojo film ID\":0, \"set in period\": 0, \"duration\": 0, \"film editor\": 0, \"FilmAffinity ID\": 0,\n",
    "                   \"Filmportal ID\": 0, \"main subject\": 0, \"composer\": 0, \"distributed by\": 0,  \"costume designer\": 0,\n",
    "                   \"production designer\": 0, \"Kinopoisk film ID\": 0, \"exploitation visa number\": 0, \"average shot length\": 0,\n",
    "                   \"original film format\": 0, \"film script\": 0, \"title\": 0, \"award\": 0, \"media franchise\": 0, \"part of the series\": 0,\n",
    "                   \"budget\": 0\n",
    "                   }\n",
    "\n",
    "        #check occurences of words in input sentence\n",
    "        for key in candidates.keys():\n",
    "            for w in candidates[key]:\n",
    "                if w in lemma_words:\n",
    "                    counter[key] += 1\n",
    "\n",
    "        max_value = max(counter, key=counter.get)\n",
    "\n",
    "        #double check for box office and avg shot length since it should appear in word combinations\n",
    "        if max_value == \"Box Office Mojo film ID\":\n",
    "            lemma_lower = [w.lower() for w in lemma_words]\n",
    "            if \"id\" not in lemma_lower:\n",
    "                max_value = \"box office\"\n",
    "\n",
    "        if max_value == \"average shot length\":\n",
    "            counter[\"average shot length\"] -= 1\n",
    "            max_value = max(counter, key=counter.get)\n",
    "        if max_value == \"original film format\":\n",
    "            counter[\"original film format\"] -= 1\n",
    "            max_value = max(counter, key=counter.get)\n",
    "\n",
    "\n",
    "        #decide between location properties:\n",
    "        if max_value in [\"country of origin\", \"filming location\", \"narrative location\"]:\n",
    "            if \"origin\" in lemma_verbs:\n",
    "                max_value = \"country of origin\"\n",
    "            elif \"film\" or \"shoot\" in lemma_verbs:\n",
    "                max_value = \"filming location\"\n",
    "            else:\n",
    "                max_value = \"narrative location\"\n",
    "\n",
    "        #decide between the production rolees:\n",
    "        if max_value in [\"production designer\", \"executive producer\"]:\n",
    "            if \"designer\" in lemma_words:\n",
    "                max_value = \"production designer\"\n",
    "            elif  \"design\" in lemma_words:\n",
    "                max_value = \"production designer\"\n",
    "            elif \"executive\" in lemma_words:\n",
    "                max_value = \"executive producer\"\n",
    "            else: max_value = \"producer\"\n",
    "\n",
    "\n",
    "        return max_value\n",
    "\n",
    "\n",
    "\n",
    "    def identify_predicate(self, question):\n",
    "\n",
    "        # Get untaggde words\n",
    "        untagged_words = question[question[\"TAG\"]=='0']\n",
    "        verbs = untagged_words[untagged_words['POS'].isin(['VP', 'VBD', 'VBG', 'VBP', 'VBZ'])]['WORD']\n",
    "        nouns = untagged_words[untagged_words['POS'].isin(['NN', 'NNP', 'NNPS', \"JJ\"])]['WORD']\n",
    "\n",
    "        #Lemmatize means to get the root of the word\n",
    "        lemma = WordNetLemmatizer()\n",
    "        lemma_verbs = [lemma.lemmatize(v) for v in verbs]\n",
    "        lemma_nouns = [lemma.lemmatize(v) for v in nouns]\n",
    "        print(lemma_nouns)\n",
    "        print(lemma_verbs)\n",
    "\n",
    "        ########################################################\n",
    "        pred = self.identity_counter(lemma_nouns = lemma_nouns, lemma_verbs=lemma_verbs)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "class TypoCorrector:\n",
    "\n",
    "    @staticmethod\n",
    "    def change_minus_sign(input):\n",
    "        # replace minus with dash\n",
    "        if \"-\" in input:\n",
    "            input = input.replace(\"-\", \"–\")\n",
    "\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def change_dash_sign(input):\n",
    "        # replace minus with dash\n",
    "        if \"-\" in input:\n",
    "            input = input.replace(\"–\",\"-\")\n",
    "\n",
    "        return input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class  ModelTrainer:\n",
    "\n",
    "    def __init__(self, ner_model, training_data, test_data):\n",
    "        self.ner_model = ner_model\n",
    "        self.x_train = pd.read_csv(training_data, encoding=\"ISO-8859-1\")\n",
    "        self.x_test = pd.read_csv(test_data, encoding=\"ISO-8859-1\")\n",
    "        print(\"Loaded data successfully\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        self.ner_model.train(self.x_train,self.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "class Crowdsourcing:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv('D:/deanh/Uni/Master/HS22/Advanced AI/Project/crowdsourcing/crowd_data.tsv', sep='\\t')\n",
    "\n",
    "    def clean_crowdsourcingfile(self):\n",
    "        #remove % sign from approvalrate to handle as integer\n",
    "        self.df[\"LifetimeApprovalRate\"] = [int(word[:-1]) for word in self.df['LifetimeApprovalRate']]\n",
    "\n",
    "        # We define upper and lower bounds for response time and approval rate by using mean and standard deviation\n",
    "        # In believe that workers outside of these bounds are malicious. We add +5% to all bounds to include valid edge cases\n",
    "        # We also drop rows that indicate as CORRECT but say the know an error and where to handle it\n",
    "\n",
    "        x = \"WorkTimeInSeconds\"\n",
    "        y = \"LifetimeApprovalRate\"\n",
    "\n",
    "        mean_approvalRate = self.df[y].mean()\n",
    "        std_approvalRate = self.df[y].std()\n",
    "\n",
    "        mean_responseTime = self.df[x].mean()\n",
    "        std_responseTime = self.df[x].std()\n",
    "\n",
    "        upperbound_responseTime = mean_responseTime + std_responseTime + 5\n",
    "        lowerbound_responseTime = mean_responseTime - std_responseTime - 5\n",
    "\n",
    "        upperbound_approvalRate = mean_approvalRate + std_approvalRate + 5\n",
    "        lowerbound_approvalRate = mean_approvalRate - std_approvalRate -5\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds > upperbound_responseTime)].index)\n",
    "        self.df = self.df.drop(self.df[(self.df.WorkTimeInSeconds < lowerbound_responseTime)].index)\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate > upperbound_approvalRate)].index)\n",
    "        self.df = self.df.drop(self.df[(self.df.LifetimeApprovalRate < lowerbound_approvalRate)].index)\n",
    "\n",
    "        self.df = self.df.drop(self.df[(self.df['AnswerLabel'] == \"CORRECT\") & (self.df[\"FixPosition\"] == \"yes\")].index)\n",
    "\n",
    "        #There is a type in Input2ID column\n",
    "        self.df[\"Input2ID\"] = self.df[\"Input2ID\"].replace(\"wdt:.P344\", \"wdt:P344\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def crowdsourcing_answer(self, sub, pred):\n",
    "        rows = self.df.loc[self.df[\"Input1ID\"] == \"wd:\"+sub]\n",
    "        rows = rows.loc[rows[\"Input2ID\"] == \"wdt:\"+pred]\n",
    "\n",
    "        agree = rows.loc[rows[\"AnswerLabel\"] == \"CORRECT\"]\n",
    "        disagree = rows.loc[rows[\"AnswerLabel\"] == \"INCORRECT\"]\n",
    "        value = rows.iloc[0]['Input3ID']\n",
    "\n",
    "        if agree.shape[0] > disagree.shape[0]:\n",
    "            return(\"The crowd agrees that the answer is \" + str(value))\n",
    "        if agree.shape[0] < disagree.shape[0]:\n",
    "            return(\"The crowd disagrees that the answer is \" + str(value))\n",
    "        else:\n",
    "            return(\"The crowd is uncertain that the answer is \" + str(value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "class KGLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_KG(self):\n",
    "        print(\"Loading knowledge graph from source\")\n",
    "        movie_graph = Graph()\n",
    "        movie_graph.parse(source='datasets/14_graph.nt', format='turtle')\n",
    "        print(\"succesfully loaded knowledge graph\")\n",
    "        return movie_graph\n",
    "\n",
    "    def load_embeddings(self, graph):\n",
    "        #add path parameter to retrive dictionaries?\n",
    "\n",
    "        DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "        RDFS = rdflib.namespace.RDFS\n",
    "        SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "\n",
    "        # load the embeddings\n",
    "        entity_emb = np.load(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'entity_embeds.npy'))\n",
    "        relation_emb = np.load(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'relation_embeds.npy'))\n",
    "\n",
    "        # load the dictionaries\n",
    "        with open(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'entity_ids.del'), 'r') as ifile:\n",
    "            ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "            id2ent = {v: k for k, v in ent2id.items()}\n",
    "        with open(os.path.join('D:\\deanh','Uni','Master','HS22','Advanced AI','Project', 'embedding_data', 'relation_ids.del'), 'r') as ifile:\n",
    "            rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "            id2rel = {v: k for k, v in rel2id.items()}\n",
    "\n",
    "        ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
    "        lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
    "\n",
    "        embedding = (entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent)\n",
    "\n",
    "        return embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# Returns a SQL statement as a string\n",
    "class QueryProducer:\n",
    "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "    WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def director_query(self, title):\n",
    "        query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:P57 ?director .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "        return query\n",
    "\n",
    "    def film_property_query(self, title, predicateId, predicateName):\n",
    "        query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:\"'''+ predicateId +'''?'''+ predicateName +''' .\n",
    "            ?\"'''+ predicateName+ '''\" rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "        return query\n",
    "\n",
    "    def actor_query(self, title):\n",
    "        query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:P57 ?actor .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "        return query\n",
    "\n",
    "    def genre_query(self, title):\n",
    "        query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:P136 ?genre .\n",
    "            ?genre rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "        return query\n",
    "\n",
    "    # returns a dict, with all film titles and their entity Id\n",
    "    def all_films_id_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "        PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX p: <http://www.wikidata.org/prop/>\n",
    "        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?item ?label\n",
    "            WHERE{\n",
    "            ?item wdt:P31 <http://www.wikidata.org/entity/Q11424> .\n",
    "            ?item rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = \"en\").\n",
    "        }\n",
    "                    '''\n",
    "        response = moviegraph.query(query)\n",
    "        id_to_film = {e[len(WD):]: str(i) for e,i in response}\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "\n",
    "        return  film_to_id\n",
    "\n",
    "        # returns a dict, with all anime titles and their entity Id\n",
    "    def all_animes_id_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "        PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX p: <http://www.wikidata.org/prop/>\n",
    "        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?item ?label\n",
    "            WHERE{\n",
    "            ?item wdt:P31 <http://www.wikidata.org/entity/Q20650540> .\n",
    "            ?item rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = \"en\").\n",
    "        }\n",
    "                    '''\n",
    "        response = moviegraph.query(query)\n",
    "        id_to_film = {e[len(WD):]: str(i) for e,i in response}\n",
    "        film_to_id = {value:key for key, value in id_to_film.items()}\n",
    "\n",
    "        # set all movie titles to lowercase\n",
    "        film_to_id = {k.lower(): v for k, v in film_to_id.items()}\n",
    "\n",
    "        return  film_to_id\n",
    "\n",
    "    def all_actors_id_and_IMDb_query(self):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "        query ='''\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "            PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX p: <http://www.wikidata.org/prop/>\n",
    "            PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "            PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "            SELECT ?item ?label ?IMDb_ID\n",
    "                WHERE{\n",
    "                ?item wdt:P31 wd:Q5 .\n",
    "                ?item wdt:P106 wd:Q33999 .\n",
    "                ?item rdfs:label ?label .\n",
    "                ?item wdt:P345 ?IMDb_ID  .\n",
    "            }\n",
    "\n",
    "\n",
    "            '''\n",
    "\n",
    "        response = moviegraph.query(query)\n",
    "        ID_to_actor = {e[len(WD):]: str(i) for e,i, IDMb in response}\n",
    "        actor_to_Id = {value:key for key, value in ID_to_actor.items()}\n",
    "\n",
    "        ID_to_actor = {e[len(WD):]: str(IDMb) for e,i, IDMb in response}\n",
    "\n",
    "        return [actor_to_Id, ID_to_actor]\n",
    "\n",
    "    def predicateID_query(self, proposition):\n",
    "        query =\"\"\"\n",
    "\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        SELECT ?obj WHERE {\n",
    "            ?obj rdfs:label \"\"\" + '\"' + proposition + '\"' + \"\"\"@en .\n",
    "\n",
    "        }\n",
    "        \"\"\"\n",
    "        entity =moviegraph.query(query)\n",
    "        df = pd.DataFrame(entity.bindings)\n",
    "        url = str((df.iloc[0][0]))\n",
    "        id = url[len(QueryProducer.WDT):]\n",
    "\n",
    "        return id\n",
    "\n",
    "    def any_property(self, title, id, propertyname):\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "        query ='''\n",
    "                SELECT ?name\n",
    "                WHERE {\n",
    "                ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                      rdfs:label \"''' + title + '''\"@en ;\n",
    "                      ns1:'''+id+''' ?'''+propertyname+''' .\n",
    "                ?director rdfs:label ?name\n",
    "                }\n",
    "                '''\n",
    "        entity =moviegraph.query(query)\n",
    "        df = pd.DataFrame(entity.bindings)\n",
    "        df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge graph from source\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-116-718b165ee07d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mKGloader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKGLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmoviegraph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKGloader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_KG\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0membedding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKGloader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_embeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmoviegraph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"loaded KG and embeddings\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-114-01e5b18f4451>\u001B[0m in \u001B[0;36mload_KG\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Loading knowledge graph from source\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0mmovie_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mGraph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0mmovie_graph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'datasets/14_graph.nt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'turtle'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"succesfully loaded knowledge graph\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mmovie_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\graph.py\u001B[0m in \u001B[0;36mparse\u001B[1;34m(self, source, publicID, format, location, file, data, **args)\u001B[0m\n\u001B[0;32m   1328\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1329\u001B[0m             \u001B[1;31m# TODO FIXME: Parser.parse should have **kwargs argument.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1330\u001B[1;33m             \u001B[0mparser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1331\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mSyntaxError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1332\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mcould_not_guess_format\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mparse\u001B[1;34m(self, source, graph, encoding, turtle)\u001B[0m\n\u001B[0;32m   1941\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mstream\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1942\u001B[0m             \u001B[0mstream\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msource\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetByteStream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1943\u001B[1;33m         \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloadStream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1944\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1945\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mprefix\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnamespace\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_bindings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mloadStream\u001B[1;34m(self, stream)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    453\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mloadStream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mIO\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mIO\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbytes\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Formula\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 454\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloadBuf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# Not ideal\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    455\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    456\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mloadBuf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbytes\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mloadBuf\u001B[1;34m(self, buf)\u001B[0m\n\u001B[0;32m    458\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartDoc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 460\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    461\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mendDoc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# self._formula\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mfeed\u001B[1;34m(self, octets)\u001B[0m\n\u001B[0;32m    484\u001B[0m                 \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 486\u001B[1;33m             \u001B[0mi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdirectiveOrStatement\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    487\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    488\u001B[0m                 \u001B[1;31m# print(\"# next char: %s\" % s[j])\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mdirectiveOrStatement\u001B[1;34m(self, argstr, h)\u001B[0m\n\u001B[0;32m    504\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckDot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    505\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 506\u001B[1;33m         \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatement\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    507\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    508\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckDot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mstatement\u001B[1;34m(self, argstr, i)\u001B[0m\n\u001B[0;32m    751\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    752\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 753\u001B[1;33m         \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproperty_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    754\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    755\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mproperty_list\u001B[1;34m(self, argstr, i, subj)\u001B[0m\n\u001B[0;32m   1094\u001B[0m             \u001B[0mi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1095\u001B[0m             \u001B[0mv\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtyping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mList\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mAny\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1096\u001B[1;33m             \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverb\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1097\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1098\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mi\u001B[0m  \u001B[1;31m# void but valid\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mverb\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m    836\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    837\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 838\u001B[1;33m         \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    839\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    840\u001B[0m             \u001B[0mres\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"->\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mprop\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m    847\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    848\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mprop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margstr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 849\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    850\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    851\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margstr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mitem\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m    850\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    851\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margstr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 852\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    853\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    854\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mblankNode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muri\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mpath\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m    857\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margstr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    858\u001B[0m         \u001B[1;34m\"\"\"Parse the path production.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 859\u001B[1;33m         \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnodeOrLiteral\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    860\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    861\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mj\u001B[0m  \u001B[1;31m# nope\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mnodeOrLiteral\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m   1457\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1458\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mnodeOrLiteral\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margstr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1459\u001B[1;33m         \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1460\u001B[0m         \u001B[0mstartline\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlines\u001B[0m  \u001B[1;31m# Remember where for error messages\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1461\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mnode\u001B[1;34m(self, argstr, i, res, subjectAlready)\u001B[0m\n\u001B[0;32m   1061\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1062\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msubj\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# If this can be a named node, then check for a name.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1063\u001B[1;33m             \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muri_ref2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1064\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1065\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36muri_ref2\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m   1177\u001B[0m         \"\"\"\n\u001B[0;32m   1178\u001B[0m         \u001B[0mqn\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtyping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mList\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mAny\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1179\u001B[1;33m         \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mqname\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1180\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mj\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1181\u001B[0m             \u001B[0mpfx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mln\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mqn\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deanh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\rdflib\\plugins\\parsers\\notation3.py\u001B[0m in \u001B[0;36mqname\u001B[1;34m(self, argstr, i, res)\u001B[0m\n\u001B[0;32m   1339\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnumberCharsPlus\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1340\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1341\u001B[1;33m         \u001B[0mlen_argstr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1342\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_notNameChars\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1343\u001B[0m             \u001B[0mj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "KGloader = KGLoader()\n",
    "moviegraph = KGloader.load_KG()\n",
    "embedding = KGloader.load_embeddings(moviegraph)\n",
    "print(\"loaded KG and embeddings\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q = QueryProducer()\n",
    "anime_dict = Q.all_animes_id_query()\n",
    "films_dict = Q.all_films_id_query()\n",
    "actors_ID_dict = Q.all_actors_id_and_IMDb_query()[0]\n",
    "actor_IMDb_dict = Q.all_actors_id_and_IMDb_query()[1]\n",
    "print(\"loaded film and actor dicts\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_frame = pd.DataFrame.from_dict(json.load(open(\"images.json\")))\n",
    "print(\"loaded image file\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C = Crowdsourcing()\n",
    "clean_crowdsourcingfile = C.clean_crowdsourcingfile()\n",
    "print(\"loaded crowdsourcing file\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Takes an argument \"training\" to determine if it should create and save a model or load an existing one\n",
    "class RunProcess:\n",
    "    def __init__(self, training, knowledgegraph):\n",
    "        print(\"Starting process\")\n",
    "        self.ner_model = NerTrainer()\n",
    "        if training:\n",
    "            self.crf_model = self.ner_model.train()\n",
    "            print(\"Created and trained model succesfully\")\n",
    "            with open('crf_model.pkl', 'wb') as f:\n",
    "                pickle.dump(self.crf_model, f)\n",
    "            print(\"Saved crf-model\")\n",
    "        else:\n",
    "            #self.ner_model = NerDLModel.pretrained('ner_mit_movie_simple_distilbert_base_cased', 'en')\\\n",
    "            #.setInputCols(['document', 'token', 'embeddings']).setOutputCol('ner')\n",
    "            with open('crf_model.pkl', 'rb') as f:\n",
    "                self.crf_model = pickle.load(f)\n",
    "            print(\"loaded ner-mnodel\")\n",
    "\n",
    "        self.question_analyser = QuestionAnalyser()\n",
    "        self.query_producer = QueryProducer()\n",
    "        print(\"Queryproduce initialized\")\n",
    "\n",
    "        self.movie_graph = knowledgegraph\n",
    "        # self.embedding = KGloader.load_embeddings(moviegraph)\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.anime_ids = anime_dict\n",
    "        self.normal_film_ids =films_dict\n",
    "        self.film_ids = {**self.anime_ids, **self.normal_film_ids} #combine both to one dict\n",
    "        print(\"created film dict\")\n",
    "\n",
    "        self.crowdourcing = Crowdsourcing()\n",
    "        self.crowdsource_file = clean_crowdsourcingfile\n",
    "        self.crowdsource_properties = C.get_properties()\n",
    "\n",
    "        self.actor_IDs = actors_ID_dict\n",
    "        self.actor_IMDb = actor_IMDb_dict\n",
    "        # self.film_ids = self.query_producer.all_films_id_query() # dictionary with film names and their IDs\n",
    "        # self.actor_IDs = self.query_producer.all_actors_id_and_IMDb_query()[0] #dict actor name to ID\n",
    "        # self.actor_IMDb = self.query_producer.all_actors_id_and_IMDb_query()[1] #dict actor ID to IMDb\n",
    "\n",
    "        print(\"waiting for image file\")\n",
    "        # self.images = pd.DataFrame.from_dict(json.load(open(\"images.json\"))) #dataframe contain image describtions and URLs to images\n",
    "        self.images = image_frame\n",
    "\n",
    "        print(\"----------Run Initialized!----------\")\n",
    "\n",
    "    #\n",
    "    def ask_question(self):\n",
    "        question = input(\"Please ask your question\")\n",
    "        return question\n",
    "\n",
    "    def identify_subject_and_predicate(self, question):\n",
    "        tagged_entities = self.ner_model.predict_tags(self.crf_model, question)\n",
    "        print(\"Sentence with tags: \")\n",
    "        print(tagged_entities)\n",
    "        print(\"#################################################################\")\n",
    "        identified_predicate = self.question_analyser.identify_predicate(tagged_entities)\n",
    "\n",
    "        #TODO Here we can also check for actors. ALSO we can check if I-title is a questions mark, comma etc.\n",
    "\n",
    "        film_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-TITLE','I-TITLE'])]\n",
    "        film_title = ''\n",
    "        for w in film_wordlist:\n",
    "            film_title = film_title + w + ' '\n",
    "\n",
    "        actor_wordlist = tagged_entities['WORD'][tagged_entities['TAG'].isin(['B-ACTOR','I-ACTOR'])]\n",
    "        actor = ''\n",
    "        for w in actor_wordlist:\n",
    "            actor = actor + w + ' '\n",
    "        # need to remove empry string at end of concatenation\n",
    "        film_title = film_title[:-1]\n",
    "        actor = actor[:-1]\n",
    "\n",
    "        print(\"Asking for: \" + identified_predicate + \" in: \" + film_title + \" or actor : \" + actor)\n",
    "        return film_title, actor, identified_predicate\n",
    "\n",
    "    def film_id_finder(self, film_title):\n",
    "        # Film entity. Using lowercase title for lookup\n",
    "        film_title = film_title.lower()\n",
    "        try:\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            return film_id\n",
    "        except:\n",
    "            print(\"could not find film, trying to find it correcting typos...\")\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.change_minus_sign(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            return film_id\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            film_title = TypoCorrector.change_dash_sign(film_title)\n",
    "            film_id = self.film_ids[film_title.lower()]\n",
    "            return film_id\n",
    "        except:\n",
    "            print(film_title)\n",
    "            raise KeyError(\"Film title contains typos or does not exist!\")\n",
    "\n",
    "        return film_id\n",
    "\n",
    "    def graph_lookup(self, film_title, pred):\n",
    "        entity = \"\"\n",
    "        if pred == \"director\":\n",
    "            query = self.query_producer.director_query(film_title)\n",
    "            entity = self.movie_graph.query(query)\n",
    "            df_entity = pd.DataFrame(entity.bindings)\n",
    "            print(\"is df epmpty?\")\n",
    "            print(df_entity.empty)\n",
    "\n",
    "            if df_entity.empty:\n",
    "                print(\"changing minus\")\n",
    "                film_title = TypoCorrector.change_minus_sign(film_title)\n",
    "                query = self.query_producer.director_query(film_title)\n",
    "                entity = self.movie_graph.query(query)\n",
    "                df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "                if df_entity.empty:\n",
    "                    print(\"changing dash\")\n",
    "                    film_title = TypoCorrector.change_dash_sign(film_title)\n",
    "                    query = self.query_producer.director_query(film_title)\n",
    "                    entity = self.movie_graph.query(query)\n",
    "                    df_entity = pd.DataFrame(entity.bindings)\n",
    "\n",
    "        # query outputs a non-string object that needs handling\n",
    "        director = ''\n",
    "        for _ in entity:\n",
    "            print(\"after graph lookup: \" + str(_.name))\n",
    "            director = str(_.name)\n",
    "        return director\n",
    "\n",
    "    def embedding_lookup(self, film_title, predicate):\n",
    "        entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent = self.embedding\n",
    "\n",
    "        # define some prefixes\n",
    "        WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "        RDFS = rdflib.namespace.RDFS\n",
    "        SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "\n",
    "        film_id = self.film_id_finder(film_title)\n",
    "        head = entity_emb[ent2id[WD[film_id]]]\n",
    "\n",
    "        if predicate =='genre':\n",
    "            # \"genre\" relation\n",
    "            pred = relation_emb[rel2id[WDT.P136]]\n",
    "        if predicate == 'rating':\n",
    "            # \"MPAA film rating\" relation\n",
    "            pred = relation_emb[rel2id[WDT.P1657]]\n",
    "        if predicate == 'screenwriter':\n",
    "            # \"screenwriter \" relation\n",
    "            pred = relation_emb[rel2id[WDT.P58]]\n",
    "\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "        # compute ranks of entities\n",
    "        ranks = dist.argsort().argsort()\n",
    "\n",
    "        top_ten_results = pd.DataFrame([\n",
    "                (id2ent[idx][len(WD):], ent2lbl[id2ent[idx]], dist[idx], rank+1)\n",
    "                for rank, idx in enumerate(most_likely[:10])],\n",
    "                columns=('Entity', 'Label', 'Score', 'Rank'))\n",
    "        print(top_ten_results)\n",
    "        best_result = top_ten_results.iloc[0][\"Label\"]\n",
    "        best_result = str(best_result)\n",
    "\n",
    "        return best_result\n",
    "\n",
    "    def image_lookup(self, name, asFilmIdentifiedName):\n",
    "        try:\n",
    "            actor_Id = self.actor_IDs[name]\n",
    "        except:\n",
    "            actor_Id = self.actor_IDs[asFilmIdentifiedName]\n",
    "\n",
    "        IMDb_Id = self.actor_IMDb[actor_Id]\n",
    "        valid_options = self.images.loc[self.images[\"cast\"].str.contains(IMDb_Id, regex=False)]\n",
    "\n",
    "        #only images where actor appears alone and its a poster. Take a random sample\n",
    "        valid_options = valid_options.loc[valid_options[\"type\"]==\"poster\"]\n",
    "        valid_options = valid_options.query(\"cast.str.len() == 1\", engine=\"python\")\n",
    "        valid_options = valid_options[\"img\"]\n",
    "        valid_options = valid_options.sample()\n",
    "\n",
    "        url = str(valid_options.iloc[0])\n",
    "        return url\n",
    "\n",
    "    def crowdsourcing_lookup(self, filmtitle, pred_id):\n",
    "        film_id = self.film_id_finder(filmtitle)\n",
    "        answer = self.crowdourcing.crowdsourcing_answer(film_id, pred_id)\n",
    "        return answer\n",
    "\n",
    "    def answer_movie_question(self):\n",
    "        question = self.ask_question()\n",
    "        film_title, actor, pred = self.identify_subject_and_predicate(question)\n",
    "\n",
    "        pred_id = self.query_producer.predicateID_query(pred)\n",
    "\n",
    "        # To perform the query, we must remove punctuations.\n",
    "\n",
    "        if len(film_title) >= 1:\n",
    "            if film_title[-1] in ['?', '!', '.']:\n",
    "                film_title = film_title[:-2]\n",
    "        # intermediate punctions have a space in front of them that needs to be removed\n",
    "        if \" :\" in film_title:\n",
    "            film_title = film_title.replace(\" :\", \":\")\n",
    "\n",
    "        if len(actor) >= 1:\n",
    "            if actor[-1] in ['?', '!', '.']:\n",
    "                actor = actor[:-2]\n",
    "\n",
    "        print(\"after identifying predicate: \" + pred)\n",
    "\n",
    "        #########################################################################\n",
    "\n",
    "        # First we try crowdsourcing lookup and see if we find an answer\n",
    "        try:\n",
    "            answer = self.crowdsourcing_lookup(film_title, pred_id)\n",
    "            print(answer)\n",
    "        except:\n",
    "            print(\"Not found in crowdsourcing\")\n",
    "\n",
    "            #No we perform graph and embedding lookup. We see if the first answer from the graph is in the top 3 of the embedding\n",
    "            #If no, we see what we will do lol\n",
    "            try:\n",
    "                answer = self.graph_lookup(film_title, pred)\n",
    "                print(answer)\n",
    "            except:\n",
    "                print(\"Not found in fuckng graph\")\n",
    "                try:\n",
    "                    answer = self.embedding_lookup(film_title, pred)\n",
    "                    print(answer)\n",
    "                except:\n",
    "                    print(\"Could not find: \" + film_title)\n",
    "                    pass\n",
    "\n",
    "\n",
    "       #TODO solve this nicer\n",
    "    #    if pred == \"director\":\n",
    "     #       director = self.graph_lookup(film_title, pred)\n",
    "      #      print(film_title+ \" was directed by \" + director)\n",
    "#\n",
    " #       if pred == 'genre':\n",
    "  #          genre = self.embedding_lookup(film_title, pred)\n",
    "   #         print('The genre of '+ film_title +' is ' + genre)\n",
    "#\n",
    " #       if pred == 'screenwriter':\n",
    "  #          screenwriter = self.embedding_lookup(film_title, pred)\n",
    "   #         print('The screenwriter of '+ film_title +' is ' + screenwriter)\n",
    "\n",
    "    #    if pred == 'rating':\n",
    "     #       rating = self.embedding_lookup(film_title, pred)\n",
    "      #      print('The MPAA rating of '+ film_title +' is ' + rating)\n",
    "#\n",
    " #       if pred == 'image':\n",
    "  #          image_url = self.image_lookup(name = actor, asFilmIdentifiedName=film_title)\n",
    "   #         print(\"Image of \" + film_title + actor + \" at this url: \"+ image_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Process3 = RunProcess(training=False, knowledgegraph=moviegraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Process3.answer_movie_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Who is the director of Good Will Hunting?\n",
    "\n",
    "Who is the screenwriter of The Masked Gang: Cyprus\n",
    "Who is the screenwriter of Titanic?\n",
    "Who is the screenwriter of Weathering with You\n",
    "\n",
    "Who is the director of Star Wars: Episode VI - Return of the Jedi?\n",
    "Who is the executive producer of X-Men: First Class?\n",
    "\n",
    "What is the MPAA film rating of Weathering with You?\n",
    "What is the MPAA film rating of Titanic?\n",
    "What is the box office of The Princess and the Frog?\n",
    "Can you tell me the publication date of Tom Meets Zizou?\n",
    "\n",
    "The Lion King\n",
    "The Notebook\n",
    "Titanic\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"(rdflib.term.Literal('James Cameron', lang='en'),)\", \"(rdflib.term.Literal('Jean Negulesco', lang='en'),)\", \"(rdflib.term.Literal('Herbert Selpin', lang='en'),)\", \"(rdflib.term.Literal('Werner Klingler', lang='en'),)\"}\n"
     ]
    }
   ],
   "source": [
    "def any_property(title, id, propertyname):\n",
    "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "    WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "    query ='''\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film ns1:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  ns1:'''+id+''' ?'''+propertyname+''' .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "            '''\n",
    "    response = moviegraph.query(query)\n",
    "    response = { str(i) for i in response}\n",
    "\n",
    "    return  response\n",
    "print(any_property(\"Titanic\", \"P57\", \"director\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keys = pd.DataFrame(dict, index=[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shit = \"x–men: first class\"\n",
    "\n",
    "ok = \"x-men: first class\"\n",
    "\n",
    "shiteee = \"-\"\n",
    "okeier = \"-\"\n",
    "print(shiteee == okeier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(keys[ok])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entity_emb, relation_emb, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "title = \"Star Wars: Episode VI – Return of the Jedi\"\n",
    "\n",
    "query ='''\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wds: <http://www.wikidata.org/entity/statement/>\n",
    "            PREFIX wdv: <http://www.wikidata.org/value/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX p: <http://www.wikidata.org/prop/>\n",
    "            PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "            PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "            SELECT ?name\n",
    "            WHERE {\n",
    "            ?film wdt:P31 <http://www.wikidata.org/entity/Q11424> ;\n",
    "                  rdfs:label \"''' + title + '''\"@en ;\n",
    "                  wdt:P57 ?director .\n",
    "            ?director rdfs:label ?name\n",
    "            }\n",
    "\n",
    "\n",
    "            '''\n",
    "\n",
    "response = moviegraph.query(query)\n",
    "response = { str(i) for i in response}\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ID_to_actor = {e[len(WD):]: str(i) for e,i, IDMb in response}\n",
    "actor_to_Id = {value:key for key, value in ID_to_actor.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "result = pd.DataFrame([\n",
    "    (id[len(WD):], label, ipbm_id)\n",
    "    for id, label, ipbm_id in response],\n",
    "    columns=('ID', \"label\", 'Imdb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = \"Lord of the Rings : the two towers\"\n",
    "\n",
    "s1 = s.replace(\" :\", \":\")\n",
    "\n",
    "print(s1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "949167c31c494cbe6f2153fb142c4e45576b141632160fdf2c3b2b305db6da2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}